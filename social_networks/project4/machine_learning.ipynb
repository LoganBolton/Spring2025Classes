{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1\n",
    "I utilized a graph convolutional network (GCN) to analyze the dataset. GCNs are well suited for this task because they effectively take into account a node's features as well as a node's neighbors. Through very limited training, I was able to get impresive results with the GCN\n",
    "\n",
    "## GCN Results\n",
    "Epoch: 250, Loss: 0.3857, Train Acc: 0.9600, Val Acc: 0.8504, Test Acc: 0.8480\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "        Node      precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.92      0.90       105\n",
    "           1       1.00      0.80      0.89        10\n",
    "           2       1.00      0.50      0.67         4\n",
    "           3       0.83      0.91      0.86        53\n",
    "           4       0.00      0.00      0.00         1\n",
    "           5       0.78      0.72      0.75        29\n",
    "           6       0.79      0.73      0.76        66\n",
    "           7       0.83      0.56      0.67         9\n",
    "           8       0.93      0.81      0.87        52\n",
    "           9       1.00      0.57      0.73         7\n",
    "          10       0.90      0.94      0.92       128\n",
    "          11       1.00      0.81      0.90        16\n",
    "          12       0.67      0.33      0.44         6\n",
    "          13       1.00      0.86      0.92         7\n",
    "          14       0.79      0.80      0.79        70\n",
    "          15       0.95      0.78      0.86        23\n",
    "          16       0.85      0.52      0.65        21\n",
    "          17       0.80      0.94      0.86       156\n",
    "\n",
    "    accuracy       _         _         0.85      763\n",
    "    macro_avg      0.83      0.69      0.75       763\n",
    "    weighted_avg   0.85      0.85      0.84       763\n",
    "\n",
    "\n",
    "ROC AUC Score (One-vs-Rest): 0.96 (very high!!)\n",
    "\n",
    "# Approach 2\n",
    "I also attempted to use simple logistic regression on this task. This trains a classifier on the matrix made up of node features.\n",
    "\n",
    "## Logistic Regression Results\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "        Node      precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.86      0.86       225\n",
    "           1       0.00      0.00      0.00         6\n",
    "           2       0.75      0.27      0.40        11\n",
    "           3       0.71      0.75      0.73        96\n",
    "           4       0.00      0.00      0.00         2\n",
    "           5       0.75      0.74      0.75        78\n",
    "           6       0.67      0.68      0.67       142\n",
    "           7       0.80      0.17      0.29        23\n",
    "           8       0.87      0.70      0.77        93\n",
    "           9       1.00      0.11      0.20         9\n",
    "          10       0.81      0.88      0.84       269\n",
    "          11       0.80      0.70      0.74        23\n",
    "          12       0.33      0.11      0.17         9\n",
    "          13       1.00      0.44      0.61        16\n",
    "          14       0.72      0.65      0.68       117\n",
    "          15       0.85      0.63      0.72        46\n",
    "          16       0.83      0.58      0.68        50\n",
    "          17       0.71      0.90      0.79       310\n",
    "\n",
    "    accuracy                           0.76      1525\n",
    "    macro avg      0.69      0.51      0.55      1525\n",
    "    weighted avg   0.77      0.76      0.75      1525\n",
    "\n",
    "One-vs-Rest ROC AUC Score: 0.92\n",
    "\n",
    "# Overall Results\n",
    "I found that the GCN was more accurate than logistic regression for this task. GCN had an accuracy of 85% and logistic regression had an accuracy of 75%. I did not perform any hyperparameter tuning and I'm sure that the performance could be boosted even higher with a properly tuned GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.7449, Train Acc: 0.6531, Val Acc: 0.6312, Test Acc: 0.6566\n",
      "Epoch: 020, Loss: 1.3136, Train Acc: 0.7685, Val Acc: 0.7415, Test Acc: 0.7523\n",
      "Epoch: 030, Loss: 1.0830, Train Acc: 0.8131, Val Acc: 0.7769, Test Acc: 0.7759\n",
      "Epoch: 040, Loss: 0.9443, Train Acc: 0.8478, Val Acc: 0.7979, Test Acc: 0.8126\n",
      "Epoch: 050, Loss: 0.8422, Train Acc: 0.8710, Val Acc: 0.8071, Test Acc: 0.8152\n",
      "Epoch: 060, Loss: 0.7702, Train Acc: 0.8852, Val Acc: 0.8163, Test Acc: 0.8126\n",
      "Epoch: 070, Loss: 0.7010, Train Acc: 0.8952, Val Acc: 0.8215, Test Acc: 0.8152\n",
      "Epoch: 080, Loss: 0.6540, Train Acc: 0.9051, Val Acc: 0.8215, Test Acc: 0.8178\n",
      "Epoch: 090, Loss: 0.6307, Train Acc: 0.9097, Val Acc: 0.8215, Test Acc: 0.8178\n",
      "Epoch: 100, Loss: 0.6074, Train Acc: 0.9139, Val Acc: 0.8281, Test Acc: 0.8244\n",
      "Epoch: 110, Loss: 0.5989, Train Acc: 0.9175, Val Acc: 0.8333, Test Acc: 0.8349\n",
      "Epoch: 120, Loss: 0.5527, Train Acc: 0.9228, Val Acc: 0.8333, Test Acc: 0.8322\n",
      "Epoch: 130, Loss: 0.5377, Train Acc: 0.9274, Val Acc: 0.8373, Test Acc: 0.8336\n",
      "Epoch: 140, Loss: 0.5311, Train Acc: 0.9320, Val Acc: 0.8386, Test Acc: 0.8401\n",
      "Epoch: 150, Loss: 0.4957, Train Acc: 0.9367, Val Acc: 0.8438, Test Acc: 0.8414\n",
      "Epoch: 160, Loss: 0.4675, Train Acc: 0.9377, Val Acc: 0.8465, Test Acc: 0.8440\n",
      "Epoch: 170, Loss: 0.4611, Train Acc: 0.9424, Val Acc: 0.8438, Test Acc: 0.8414\n",
      "Epoch: 180, Loss: 0.4435, Train Acc: 0.9454, Val Acc: 0.8504, Test Acc: 0.8453\n",
      "Epoch: 190, Loss: 0.4272, Train Acc: 0.9505, Val Acc: 0.8491, Test Acc: 0.8388\n",
      "Epoch: 200, Loss: 0.4083, Train Acc: 0.9513, Val Acc: 0.8543, Test Acc: 0.8414\n",
      "Epoch: 210, Loss: 0.3904, Train Acc: 0.9551, Val Acc: 0.8543, Test Acc: 0.8388\n",
      "Epoch: 220, Loss: 0.3995, Train Acc: 0.9587, Val Acc: 0.8530, Test Acc: 0.8427\n",
      "Epoch: 230, Loss: 0.3779, Train Acc: 0.9572, Val Acc: 0.8530, Test Acc: 0.8467\n",
      "Epoch: 240, Loss: 0.3730, Train Acc: 0.9598, Val Acc: 0.8517, Test Acc: 0.8453\n",
      "Epoch: 250, Loss: 0.3857, Train Acc: 0.9600, Val Acc: 0.8504, Test Acc: 0.8480\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load Data\n",
    "# ---------------------------\n",
    "features_path = '/Users/log/Github/Spring2025Classes/social_networks/project4/lasftm_asia/lastfm_asia_features.json'\n",
    "targets_path = '/Users/log/Github/Spring2025Classes/social_networks/project4/lasftm_asia/lastfm_asia_target.csv'\n",
    "edges_path   = '/Users/log/Github/Spring2025Classes/social_networks/project4/lasftm_asia/lastfm_asia_edges.csv'\n",
    "\n",
    "# Load node features from JSON.\n",
    "with open(features_path, 'r') as f:\n",
    "    features_dict = json.load(f)\n",
    "\n",
    "# Convert the features dictionary into a sorted list (assuming keys are node IDs as strings).\n",
    "node_ids = sorted(features_dict.keys(), key=lambda x: int(x))\n",
    "features_list = [features_dict[node_id] for node_id in node_ids]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Create a Multi-hot Encoding for Features\n",
    "# ---------------------------\n",
    "# Build a set of all unique artist IDs\n",
    "all_artists = set()\n",
    "for feats in features_list:\n",
    "    all_artists.update(feats)\n",
    "all_artists = sorted(all_artists)\n",
    "artist_to_index = {artist: i for i, artist in enumerate(all_artists)}\n",
    "\n",
    "# Create a fixed-dimension feature matrix: (num_nodes, num_artists)\n",
    "num_nodes = len(features_list)\n",
    "num_artists = len(artist_to_index)\n",
    "features_matrix = np.zeros((num_nodes, num_artists), dtype=np.float32)\n",
    "\n",
    "for i, feats in enumerate(features_list):\n",
    "    for artist in feats:\n",
    "        features_matrix[i, artist_to_index[artist]] = 1\n",
    "\n",
    "# Convert the numpy array to a torch tensor\n",
    "x = torch.tensor(features_matrix, dtype=torch.float)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Load Targets and Edges\n",
    "# ---------------------------\n",
    "# Load targets. Assumes CSV has columns 'id' and 'target'.\n",
    "targets_df = pd.read_csv(targets_path)\n",
    "y = torch.tensor(targets_df['target'].values, dtype=torch.long)\n",
    "\n",
    "# Load edges. Assumes CSV has columns 'node_1' and 'node_2'.\n",
    "edges_df = pd.read_csv(edges_path)\n",
    "edge_index = torch.tensor(edges_df[['node_1', 'node_2']].values.T, dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Create the PyG Data Object and Data Splits\n",
    "# ---------------------------\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create boolean masks for train/validation/test splits (80/10/10)\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "perm = torch.randperm(data.num_nodes)\n",
    "train_idx = perm[:int(0.8 * data.num_nodes)]\n",
    "val_idx   = perm[int(0.8 * data.num_nodes):int(0.9 * data.num_nodes)]\n",
    "test_idx  = perm[int(0.9 * data.num_nodes):]\n",
    "\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Define the GCN Model\n",
    "# ---------------------------\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Training Setup\n",
    "# ---------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(in_channels=data.num_features,\n",
    "            hidden_channels=16,\n",
    "            out_channels=len(torch.unique(data.y))).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[mask] == data.y[mask]).sum().item()\n",
    "        acc = correct / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Train the Model\n",
    "# ---------------------------\n",
    "for epoch in range(1, 251):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc = evaluate(data.train_mask)\n",
    "        val_acc = evaluate(data.val_mask)\n",
    "        test_acc = evaluate(data.test_mask)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Extended Evaluation Metrics on the Test Set\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    # Since our model outputs log probabilities, we convert them to probabilities\n",
    "    prob_test = torch.exp(out[data.test_mask]).cpu().numpy()\n",
    "    pred_test = np.argmax(prob_test, axis=1)\n",
    "    true_test = data.y[data.test_mask].cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extended Evaluation Metrics ---\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 97   0   0   0   0   0   2   0   0   0   1   0   0   0   2   0   0   3]\n",
      " [  0   8   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   2   0   0   0   0   0   0   0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0  48   0   1   1   1   0   0   0   0   1   0   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0  21   0   0   0   0   0   0   0   0   3   0   0   4]\n",
      " [  4   0   0   1   0   1  48   0   0   0   2   0   0   0   6   0   1   3]\n",
      " [  0   0   0   0   0   0   0   5   0   0   2   0   0   0   0   0   0   2]\n",
      " [  0   0   0   2   0   0   1   0  42   0   1   0   0   0   0   0   1   5]\n",
      " [  0   0   0   0   0   0   1   0   0   4   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   2   0   0   0 120   0   0   0   1   1   0   4]\n",
      " [  1   0   0   0   0   0   0   0   0   0   2  13   0   0   0   0   0   0]\n",
      " [  0   0   0   3   0   0   0   0   0   0   1   0   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   6   0   0   0   0]\n",
      " [  3   0   0   1   0   1   2   0   1   0   0   0   0   0  56   0   0   6]\n",
      " [  1   0   0   1   0   0   1   0   1   0   0   0   0   0   0  18   0   1]\n",
      " [  1   0   0   0   0   1   0   0   0   0   0   0   0   0   1   0  11   7]\n",
      " [  3   0   0   0   0   2   3   0   1   0   0   0   0   0   1   0   0 146]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.90       105\n",
      "           1       1.00      0.80      0.89        10\n",
      "           2       1.00      0.50      0.67         4\n",
      "           3       0.83      0.91      0.86        53\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.78      0.72      0.75        29\n",
      "           6       0.79      0.73      0.76        66\n",
      "           7       0.83      0.56      0.67         9\n",
      "           8       0.93      0.81      0.87        52\n",
      "           9       1.00      0.57      0.73         7\n",
      "          10       0.90      0.94      0.92       128\n",
      "          11       1.00      0.81      0.90        16\n",
      "          12       0.67      0.33      0.44         6\n",
      "          13       1.00      0.86      0.92         7\n",
      "          14       0.79      0.80      0.79        70\n",
      "          15       0.95      0.78      0.86        23\n",
      "          16       0.85      0.52      0.65        21\n",
      "          17       0.80      0.94      0.86       156\n",
      "\n",
      "    accuracy                           0.85       763\n",
      "   macro avg       0.83      0.69      0.75       763\n",
      "weighted avg       0.85      0.85      0.84       763\n",
      "\n",
      "\n",
      "ROC AUC Score (One-vs-Rest): 0.9610364123132078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import additional metrics from scikit-learn\n",
    "\n",
    "print(\"\\n--- Extended Evaluation Metrics ---\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(true_test, pred_test))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_test, pred_test))\n",
    "\n",
    "# For ROC AUC, we need to binarize the true labels.\n",
    "num_classes = len(torch.unique(data.y))\n",
    "true_test_binarized = label_binarize(true_test, classes=list(range(num_classes)))\n",
    "\n",
    "try:\n",
    "    roc_auc = roc_auc_score(true_test_binarized, prob_test, multi_class='ovr')\n",
    "    print(\"\\nROC AUC Score (One-vs-Rest):\", roc_auc)\n",
    "except Exception as e:\n",
    "    print(\"\\nCould not compute ROC AUC Score:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted features to a binary matrix of shape: (7624, 7842)\n",
      "Accuracy: 0.7632786885245901\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       225\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.75      0.27      0.40        11\n",
      "           3       0.71      0.75      0.73        96\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.75      0.74      0.75        78\n",
      "           6       0.67      0.68      0.67       142\n",
      "           7       0.80      0.17      0.29        23\n",
      "           8       0.87      0.70      0.77        93\n",
      "           9       1.00      0.11      0.20         9\n",
      "          10       0.81      0.88      0.84       269\n",
      "          11       0.80      0.70      0.74        23\n",
      "          12       0.33      0.11      0.17         9\n",
      "          13       1.00      0.44      0.61        16\n",
      "          14       0.72      0.65      0.68       117\n",
      "          15       0.85      0.63      0.72        46\n",
      "          16       0.83      0.58      0.68        50\n",
      "          17       0.71      0.90      0.79       310\n",
      "\n",
      "    accuracy                           0.76      1525\n",
      "   macro avg       0.69      0.51      0.55      1525\n",
      "weighted avg       0.77      0.76      0.75      1525\n",
      "\n",
      "One-vs-Rest ROC AUC Score: 0.9241175188198765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- Load Node Features ---\n",
    "# The JSON file contains a dictionary mapping node IDs (as strings) to lists of artist IDs.\n",
    "features_path = '/Users/log/Github/Spring2025Classes/social_networks/project4/lasftm_asia/lastfm_asia_features.json'\n",
    "with open(features_path, 'r') as f:\n",
    "    features_dict = json.load(f)\n",
    "\n",
    "# Convert keys to integers and sort by node id for consistency.\n",
    "features_dict = {int(k): v for k, v in features_dict.items()}\n",
    "features_series = pd.Series(features_dict).sort_index()\n",
    "\n",
    "# --- Convert Features to a Binary Matrix ---\n",
    "# Here we treat each nodeâ€™s list of liked artists as a set of labels\n",
    "# and convert it into a multi-hot (binary) feature vector.\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "X = mlb.fit_transform(features_series)\n",
    "print(f\"Converted features to a binary matrix of shape: {X.shape}\")\n",
    "\n",
    "# --- Load Node Targets ---\n",
    "# The CSV file contains two columns: 'id' and 'target'.\n",
    "targets_path = '/Users/log/Github/Spring2025Classes/social_networks/project4/lasftm_asia/lastfm_asia_target.csv'\n",
    "targets_df = pd.read_csv(targets_path)\n",
    "targets_df.set_index('id', inplace=True)\n",
    "\n",
    "# Align targets with the order of node IDs in our features.\n",
    "targets_df = targets_df.loc[features_series.index]\n",
    "y = targets_df['target']\n",
    "\n",
    "# --- Split the Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Train a Classifier ---\n",
    "# We use Logistic Regression which can work directly with sparse input.\n",
    "clf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate the Model ---\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Compute the ROC AUC Score (One-vs-Rest) ---\n",
    "# Binarize the true labels\n",
    "classes = sorted(y.unique())\n",
    "y_test_binarized = label_binarize(y_test, classes=classes)\n",
    "\n",
    "# Get the predicted probabilities for each class\n",
    "y_score = clf.predict_proba(X_test)\n",
    "\n",
    "# Compute the ROC AUC score using One-vs-Rest strategy\n",
    "roc_auc = roc_auc_score(y_test_binarized, y_score, multi_class=\"ovr\", average=\"macro\")\n",
    "print(\"One-vs-Rest ROC AUC Score:\", roc_auc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
