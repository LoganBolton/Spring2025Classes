output <- paste(
"Graph Summary",
paste("Number of nodes:", num_nodes),
paste("Number of edges:", num_edges),
paste("Density:", round(edge_density(g), 6)),
paste(""),
"Strong Connectivity",
paste("Number of strongly connected components:", components$no),
paste("Size of largest strongly connected component:", max(components$csize)),
paste("fraction of elements belonging to the largest strong subcomponent: ", max(components$csize)/num_nodes),
"The graph is NOT strongly connected",
paste("Largest diameter:", diameter_largest, "\n"),
paste("Largest radius:", radius_largest, "\n"),
"-------------------------------------------------------------------",
sep = "\n"
)
cat(output)
weights <- E(g)$weight
hist(weights,
breaks = 50,
main = "Distribution of Email Weights",
xlab = "Number of Emails (Weight)",
col = "skyblue")
# Alternatively, view the frequency of each weight
table(weights)
# Read the text file into a dataframe
df <- read.table("email-Eu-core.txt", header = FALSE, sep = " ")
colnames(df) <- c("From", "To")
# The dataset does not hav
# df_weighted <- aggregate(weight ~ From + To, data = transform(df, weight = 1), FUN = sum)
g <- graph_from_data_frame(df, directed = TRUE)
# g <- graph_from_data_frame(df_weighted, directed = TRUE)
num_nodes = vcount(g)
num_edges = ecount(g)
# Extract the vertices belonging to the largest strongly connected component
largest_comp_vertices <- V(g)[components$membership == which.max(components$csize)]
g_largest <- induced_subgraph(g, largest_comp_vertices)
# Calculate diameter and radius on the largest SCC
diameter_largest <- diameter(g_largest, directed = TRUE)
radius_largest <- radius(g_largest, mode = "out")
# Check strong connectivity
components <- components(g, mode="strong")
output <- paste(
"Graph Summary",
paste("Number of nodes:", num_nodes),
paste("Number of edges:", num_edges),
paste("Density:", round(edge_density(g), 6)),
paste(""),
"Strong Connectivity",
paste("Number of strongly connected components:", components$no),
paste("Size of largest strongly connected component:", max(components$csize)),
paste("fraction of elements belonging to the largest strong subcomponent: ", max(components$csize)/num_nodes),
"The graph is NOT strongly connected",
paste("Largest diameter:", diameter_largest),
paste("Largest radius:", radius_largest),
"-------------------------------------------------------------------",
sep = "\n"
)
cat(output)
# Read the text file into a dataframe
df <- read.table("email-Eu-core.txt", header = FALSE, sep = " ")
colnames(df) <- c("From", "To")
# The dataset does not hav
# df_weighted <- aggregate(weight ~ From + To, data = transform(df, weight = 1), FUN = sum)
g <- graph_from_data_frame(df, directed = TRUE)
# g <- graph_from_data_frame(df_weighted, directed = TRUE)
num_nodes = vcount(g)
num_edges = ecount(g)
# Extract the vertices belonging to the largest strongly connected component
largest_comp_vertices <- V(g)[components$membership == which.max(components$csize)]
g_largest <- induced_subgraph(g, largest_comp_vertices)
# Calculate diameter and radius on the largest SCC
diameter_largest <- diameter(g_largest, directed = TRUE)
radius_largest <- radius(g_largest, mode = "out")
# Check strong connectivity
components <- components(g, mode="strong")
output <- paste(
"Graph Summary",
paste("Number of nodes:", num_nodes),
paste("Number of edges:", num_edges),
paste("Density:", round(edge_density(g), 6)),
paste(""),
"Strong Connectivity",
paste("Number of strongly connected components:", components$no),
paste("Size of largest strongly connected component:", max(components$csize)),
paste("fraction of elements belonging to the largest strong subcomponent: ", max(components$csize)/num_nodes),
"The graph is NOT strongly connected",
paste("Largest diameter:", diameter_largest),
paste("Largest radius:", radius_largest),
"-------------------------------------------------------------------",
sep = "\n"
)
cat(output)
# Read the text file into a dataframe
df <- read.table("email-Eu-core.txt", header = FALSE, sep = " ")
colnames(df) <- c("From", "To")
# The dataset does not hav
# df_weighted <- aggregate(weight ~ From + To, data = transform(df, weight = 1), FUN = sum)
g <- graph_from_data_frame(df, directed = TRUE)
# g <- graph_from_data_frame(df_weighted, directed = TRUE)
num_nodes = vcount(g)
num_edges = ecount(g)
# Check strong connectivity
components <- components(g, mode="strong")
# Extract the vertices belonging to the largest strongly connected component
largest_comp_vertices <- V(g)[components$membership == which.max(components$csize)]
g_largest <- induced_subgraph(g, largest_comp_vertices)
# Calculate diameter and radius on the largest SCC
diameter_largest <- diameter(g_largest, directed = TRUE)
radius_largest <- radius(g_largest, mode = "out")
output <- paste(
"Graph Summary",
paste("Number of nodes:", num_nodes),
paste("Number of edges:", num_edges),
paste("Density:", round(edge_density(g), 6)),
paste(""),
"Strong Connectivity",
paste("Number of strongly connected components:", components$no),
paste("Size of largest strongly connected component:", max(components$csize)),
paste("fraction of elements belonging to the largest strong subcomponent: ", max(components$csize)/num_nodes),
"The graph is NOT strongly connected",
paste("Largest diameter:", diameter_largest),
paste("Largest radius:", radius_largest),
"-------------------------------------------------------------------",
sep = "\n"
)
cat(output)
# Read the text file into a dataframe
df <- read.table("email-Eu-core.txt", header = FALSE, sep = " ")
colnames(df) <- c("From", "To")
# The dataset does not hav
# df_weighted <- aggregate(weight ~ From + To, data = transform(df, weight = 1), FUN = sum)
g <- graph_from_data_frame(df, directed = TRUE)
# g <- graph_from_data_frame(df_weighted, directed = TRUE)
num_nodes = vcount(g)
num_edges = ecount(g)
# Check strong connectivity
components <- components(g, mode="strong")
# Extract the vertices belonging to the largest strongly connected component
largest_comp_vertices <- V(g)[components$membership == which.max(components$csize)]
g_largest <- induced_subgraph(g, largest_comp_vertices)
# Calculate diameter and radius on the largest SCC
diameter_largest <- diameter(g_largest, directed = TRUE)
radius_largest <- radius(g_largest, mode = "out")
output <- paste(
"Graph Summary",
paste("Number of nodes:", num_nodes),
paste("Number of edges:", num_edges),
paste("Density:", round(edge_density(g), 6)),
paste(""),
"Strong Connectivity",
paste("Number of strongly connected components:", components$no),
paste("Size of largest strongly connected component:", max(components$csize)),
paste("fraction of elements belonging to the largest strong subcomponent: ", max(components$csize)/num_nodes),
"The graph is NOT strongly connected",
paste("Largest diameter:", diameter_largest),
paste("Largest radius:", radius_largest),
"-------------------------------------------------------------------",
sep = "\n"
)
cat(output)
adj_matrix <- as_adjacency_matrix(g, sparse = FALSE)
# Compute clustering coefficients (cycle type)
clustering_coeff <- ClustF(adj_matrix, type = "cycle")
# Print node-level cycle clustering coefficients
print(clustering_coeff$cycleCC)
# Print the global cycle clustering coefficient
print(clustering_coeff$GlobalcycleCC)
# Optionally, you can also print the out-clustering coefficients:
print("Node-level out-clustering coefficients:")
print(clustering_coeff$outCC)
print(paste("Global out-clustering coefficient:", clustering_coeff$GlobaloutCC))
adj_matrix <- as_adjacency_matrix(g, sparse = FALSE)
# Compute clustering coefficients (cycle type)
clustering_coeff <- ClustF(adj_matrix, type = "cycle")
# Print node-level cycle clustering coefficients
# print(clustering_coeff$cycleCC)
# Print the global cycle clustering coefficient
print(clustering_coeff$GlobalcycleCC)
# Optionally, you can also print the out-clustering coefficients:
# print("Node-level out-clustering coefficients:")
# print(clustering_coeff$outCC)
print(paste("Global out-clustering coefficient:", clustering_coeff$GlobaloutCC))
adj_matrix <- as_adjacency_matrix(g, sparse = FALSE)
# Compute clustering coefficients (cycle type)
clustering_coeff <- ClustF(adj_matrix, type = "cycle")
# Print node-level cycle clustering coefficients
# print(clustering_coeff$cycleCC)
# Print the global cycle clustering coefficient
print(clustering_coeff$GlobalcycleCC)
# Optionally, you can also print the out-clustering coefficients:
# print("Node-level out-clustering coefficients:")
# print(clustering_coeff$outCC)
print(paste("Global out-clustering coefficient:", clustering_coeff$GlobaloutCC))
```{r degree_distribution, echo=TRUE, fig.width=10, fig.height=4}
# Calculate in-degree, out-degree, and total degree
in_degree <- degree(g, mode = "in")
out_degree <- degree(g, mode = "out")
total_degree <- degree(g, mode = "all")
# Set up a 1x3 plotting layout
par(mfrow = c(1, 3))
# Plot histogram for in-degree
hist(in_degree, breaks = 50, main = "In-Degree Distribution",
xlab = "In-Degree", ylab = "Frequency", col = "skyblue", border = "black")
# Plot histogram for out-degree
hist(out_degree, breaks = 50, main = "Out-Degree Distribution",
xlab = "Out-Degree", ylab = "Frequency", col = "salmon", border = "black")
# Plot histogram for total degree
hist(total_degree, breaks = 50, main = "Total Degree Distribution",
xlab = "Total Degree", ylab = "Frequency", col = "lightgreen", border = "black")
# Reset plotting layout
par(mfrow = c(1, 1))
# Summary statistics
cat("In-Degree Summary:\n")
print(summary(in_degree))
cat("\nOut-Degree Summary:\n")
print(summary(out_degree))
cat("\nTotal Degree Summary:\n")
print(summary(total_degree))
eb_communities <- cluster_edge_betweenness(g)
eb_membership <- membership(eb_communities)
eb_modularity <- modularity(eb_communities)
eb_sizes <- sizes(eb_communities)
# Walktrap Community Detection
wt_communities <- cluster_walktrap(g)
wt_membership <- membership(wt_communities)
wt_modularity <- modularity(wt_communities)
wt_sizes <- sizes(wt_communities)
# Print summary statistics for both algorithms
cat("Edge Betweenness Community Detection:\n")
cat("Number of communities:", length(unique(eb_membership)), "\n")
cat("Modularity:", round(eb_modularity, 4), "\n")
cat("Community sizes:\n")
print(sort(table(eb_membership), decreasing = TRUE)[1:10])
cat("\n")
cat("Walktrap Community Detection:\n")
cat("Number of communities:", length(unique(wt_membership)), "\n")
cat("Modularity:", round(wt_modularity, 4), "\n")
cat("Community sizes:\n")
print(sort(table(wt_membership), decreasing = TRUE)[1:10])
# Visualize the communities
# Function to plot communities with layout optimization
plot_communities <- function(graph, membership, title) {
# Create color palette for communities
num_communities <- length(unique(membership))
colors <- rainbow(num_communities)
# Set vertex colors based on community membership
V(graph)$color <- colors[membership]
# Calculate layout (using Fruchterman-Reingold algorithm)
layout <- layout_with_fr(graph)
# Plot the graph
plot(graph,
layout = layout,
vertex.size = 3,
vertex.label = NA,
edge.arrow.size = 0.2,
main = title)
}
# Set up plotting area for side-by-side comparison
par(mfrow = c(1, 2))
# Plot both community structures
plot_communities(g, eb_membership, "Edge Betweenness Communities")
plot_communities(g, wt_membership, "Walktrap Communities")
# Reset plotting parameters
par(mfrow = c(1, 1))
print(eb_membership)
global_triad <- triad.census(g)
cat("Global Triad Census for G:\n")
print(global_triad)
# Triad census for communities detected with Walktrap
community_ids <- unique(wt_membership)
community_triad <- list()
for (comm in community_ids) {
# Get the node names for the current community
nodes_in_comm <- names(wt_membership)[wt_membership == comm]
subgraph_comm <- induced_subgraph(g, vids = nodes_in_comm)
# triad.census requires at least 3 nodes to be meaningful.
if (vcount(subgraph_comm) >= 3) {
community_triad[[as.character(comm)]] <- triad.census(subgraph_comm)
} else {
community_triad[[as.character(comm)]] <- "Too few nodes for triad census"
}
}
# Print the triad census for each community
cat("\nTriad Census for each community (Walktrap):\n")
for (comm in names(community_triad)) {
cat("Community", comm, ":\n")
print(community_triad[[comm]])
cat("\n")
}
eb_communities <- cluster_edge_betweenness(g)
eb_communities <- cluster_edge_betweenness(g)
# Calculate in-degree, out-degree, and total degree
in_degree <- degree(g, mode = "in")
out_degree <- degree(g, mode = "out")
total_degree <- degree(g, mode = "all")
# Set up a 1x3 plotting layout
par(mfrow = c(1, 3))
# Plot histogram for in-degree
hist(in_degree, breaks = 50, main = "In-Degree Distribution",
xlab = "In-Degree", ylab = "Frequency", col = "skyblue", border = "black")
# Plot histogram for out-degree
hist(out_degree, breaks = 50, main = "Out-Degree Distribution",
xlab = "Out-Degree", ylab = "Frequency", col = "salmon", border = "black")
# Plot histogram for total degree
hist(total_degree, breaks = 50, main = "Total Degree Distribution",
xlab = "Total Degree", ylab = "Frequency", col = "lightgreen", border = "black")
# Reset plotting layout
par(mfrow = c(1, 1))
# Summary statistics
cat("In-Degree Summary:\n")
print(summary(in_degree))
cat("\nOut-Degree Summary:\n")
print(summary(out_degree))
cat("\nTotal Degree Summary:\n")
print(summary(total_degree))
# Calculate in-degree, out-degree, and total degree
in_degree <- degree(g, mode = "in")
out_degree <- degree(g, mode = "out")
total_degree <- degree(g, mode = "all")
# Set up a 1x3 plotting layout
par(mfrow = c(1, 3))
# Plot histogram for in-degree
hist(in_degree, breaks = 50, main = "In-Degree Distribution",
xlab = "In-Degree", ylab = "Frequency", col = "skyblue", border = "black")
# Plot histogram for out-degree
hist(out_degree, breaks = 50, main = "Out-Degree Distribution",
xlab = "Out-Degree", ylab = "Frequency", col = "salmon", border = "black")
# Plot histogram for total degree
hist(total_degree, breaks = 50, main = "Total Degree Distribution",
xlab = "Total Degree", ylab = "Frequency", col = "lightgreen", border = "black")
# Reset plotting layout
par(mfrow = c(1, 1))
# Summary statistics
cat("In-Degree Summary:\n")
print(summary(in_degree))
cat("\nOut-Degree Summary:\n")
print(summary(out_degree))
cat("\nTotal Degree Summary:\n")
print(summary(total_degree))
print(eb_communities)
# Print summary statistics for both algorithms
cat("Edge Betweenness Community Detection:\n")
cat("Number of communities:", length(unique(eb_membership)), "\n")
cat("Modularity:", round(eb_modularity, 4), "\n")
cat("Community sizes:\n")
print(sort(table(eb_membership), decreasing = TRUE)[1:10])
cat("\n")
cat("Walktrap Community Detection:\n")
cat("Number of communities:", length(unique(wt_membership)), "\n")
cat("Modularity:", round(wt_modularity, 4), "\n")
cat("Community sizes:\n")
print(sort(table(wt_membership), decreasing = TRUE)[1:10])
# Visualize the communities
# Function to plot communities with layout optimization
plot_communities <- function(graph, membership, title) {
# Create color palette for communities
num_communities <- length(unique(membership))
colors <- rainbow(num_communities)
# Set vertex colors based on community membership
V(graph)$color <- colors[membership]
# Calculate layout (using Fruchterman-Reingold algorithm)
layout <- layout_with_fr(graph)
# Plot the graph
plot(graph,
layout = layout,
vertex.size = 3,
vertex.label = NA,
edge.arrow.size = 0.2,
main = title)
}
# Set up plotting area for side-by-side comparison
par(mfrow = c(1, 2))
# Plot both community structures
plot_communities(g, eb_membership, "Edge Betweenness Communities")
plot_communities(g, wt_membership, "Walktrap Communities")
# Reset plotting parameters
par(mfrow = c(1, 1))
# Function to analyze triads in relation to communities
analyze_triads_communities <- function(g, communities) {
# Initialize counters for different triad types
triad_counts <- list(
same_comm = c(type0 = 0, type1 = 0, type2 = 0, type3 = 0),
diff_comm = c(type0 = 0, type1 = 0, type2 = 0, type3 = 0)
)
# Get all possible triads
node_combinations <- combn(V(g), 3)
for(i in 1:ncol(node_combinations)) {
nodes <- node_combinations[,i]
# Count edges in triad
subg <- induced_subgraph(g, nodes)
edge_count <- ecount(subg)
# Check if all nodes are in same community
comm_memberships <- membership(communities)[nodes]
same_community <- length(unique(comm_memberships)) == 1
# Update counts
if(same_community) {
triad_counts$same_comm[edge_count + 1] <- triad_counts$same_comm[edge_count + 1] + 1
} else {
triad_counts$diff_comm[edge_count + 1] <- triad_counts$diff_comm[edge_count + 1] + 1
}
}
return(triad_counts)
}
# Analyze triads for Walktrap communities
triad_analysis <- analyze_triads_communities(g, wt_communities)
# Function to analyze triads in relation to communities
analyze_triads_communities <- function(g, communities) {
# Initialize counters for different triad types
triad_counts <- list(
same_comm = c(type0 = 0, type1 = 0, type2 = 0, type3 = 0),
diff_comm = c(type0 = 0, type1 = 0, type2 = 0, type3 = 0)
)
# Get all possible triads
node_combinations <- combn(V(g), 3)
total_triads <- ncol(node_combinations)
# Add progress monitoring
pb <- txtProgressBar(min = 0, max = total_triads, style = 3)
# Add timing
start_time <- Sys.time()
for(i in 1:total_triads) {
nodes <- node_combinations[,i]
# Count edges in triad
subg <- induced_subgraph(g, nodes)
edge_count <- ecount(subg)
# Check if all nodes are in same community
comm_memberships <- membership(communities)[nodes]
same_community <- length(unique(comm_memberships)) == 1
# Update counts
if(same_community) {
triad_counts$same_comm[edge_count + 1] <- triad_counts$same_comm[edge_count + 1] + 1
} else {
triad_counts$diff_comm[edge_count + 1] <- triad_counts$diff_comm[edge_count + 1] + 1
}
# Update progress bar every 1000 iterations
if(i %% 1000 == 0) {
setTxtProgressBar(pb, i)
}
}
close(pb)
end_time <- Sys.time()
# Calculate additional statistics
total_same_comm <- sum(triad_counts$same_comm)
total_diff_comm <- sum(triad_counts$diff_comm)
# Add proportions and timing to results
results <- list(
counts = triad_counts,
summary = list(
total_triads = total_triads,
same_comm_prop = triad_counts$same_comm / total_same_comm,
diff_comm_prop = triad_counts$diff_comm / total_diff_comm,
execution_time = difftime(end_time, start_time, units = "mins")
)
)
return(results)
}
# Analyze triads for Walktrap communities
triad_analysis <- analyze_triads_communities(g, wt_communities)
# Function to analyze triads in relation to communities
analyze_triads_communities <- function(g, communities) {
# Initialize counters for different triad types
triad_counts <- list(
same_comm = c(type0 = 0, type1 = 0, type2 = 0, type3 = 0),
diff_comm = c(type0 = 0, type1 = 0, type2 = 0, type3 = 0)
)
cat("test")
# Get all possible triads
node_combinations <- combn(V(g), 3)
total_triads <- ncol(node_combinations)
# Add progress monitoring
pb <- txtProgressBar(min = 0, max = total_triads, style = 3)
# Add timing
start_time <- Sys.time()
for(i in 1:total_triads) {
nodes <- node_combinations[,i]
# Count edges in triad
subg <- induced_subgraph(g, nodes)
edge_count <- ecount(subg)
# Check if all nodes are in same community
comm_memberships <- membership(communities)[nodes]
same_community <- length(unique(comm_memberships)) == 1
# Update counts
if(same_community) {
triad_counts$same_comm[edge_count + 1] <- triad_counts$same_comm[edge_count + 1] + 1
} else {
triad_counts$diff_comm[edge_count + 1] <- triad_counts$diff_comm[edge_count + 1] + 1
}
# Update progress bar every 1000 iterations
if(i %% 1000 == 0) {
setTxtProgressBar(pb, i)
}
}
close(pb)
end_time <- Sys.time()
# Calculate additional statistics
total_same_comm <- sum(triad_counts$same_comm)
total_diff_comm <- sum(triad_counts$diff_comm)
# Add proportions and timing to results
results <- list(
counts = triad_counts,
summary = list(
total_triads = total_triads,
same_comm_prop = triad_counts$same_comm / total_same_comm,
diff_comm_prop = triad_counts$diff_comm / total_diff_comm,
execution_time = difftime(end_time, start_time, units = "mins")
)
)
return(results)
}
# Analyze triads for Walktrap communities
triad_analysis <- analyze_triads_communities(g, wt_communities)
sessionInfo()
