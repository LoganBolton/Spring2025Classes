# Project Summary

## Research Question
_Is it possible to reverse engineer properties of a text prompt based off the attention map of an LLM?_

## Related Work
Other Arxiv papers have shown that you can train a transformer to reverse engineer the actual text input with okayish accuracy just based off the attention values across all heads/layers


# Method

## Experiment
- Feed in a directed graph to an LLM in a simple text format. 
- 