[
  {
    "graph_id": 0,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      2,
      2,
      2,
      3,
      4,
      4
    ],
    "target": [
      0,
      4,
      0,
      1,
      3,
      4,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "B->A\nB->E\nC->A\nC->B\nC->D\nD->E\nE->A\nE->B\n",
    "averaged_attention_matrix_path": "averaged_id_0.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_0_0.npy",
      "attention_matrices/demo1/avg_attn_0_1.npy",
      "attention_matrices/demo1/avg_attn_0_2.npy",
      "attention_matrices/demo1/avg_attn_0_3.npy",
      "attention_matrices/demo1/avg_attn_0_4.npy",
      "attention_matrices/demo1/avg_attn_0_5.npy",
      "attention_matrices/demo1/avg_attn_0_6.npy",
      "attention_matrices/demo1/avg_attn_0_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 1,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      4,
      5
    ],
    "target": [
      1,
      1,
      0,
      2,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->B\nC->B\nD->A\nE->C\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_1.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_1_0.npy",
      "attention_matrices/demo1/avg_attn_1_1.npy",
      "attention_matrices/demo1/avg_attn_1_2.npy",
      "attention_matrices/demo1/avg_attn_1_3.npy",
      "attention_matrices/demo1/avg_attn_1_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 2,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      2,
      4,
      5,
      5,
      5
    ],
    "target": [
      4,
      2,
      0,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "C->E\nE->C\nF->A\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_2.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_2_0.npy",
      "attention_matrices/demo1/avg_attn_2_1.npy",
      "attention_matrices/demo1/avg_attn_2_2.npy",
      "attention_matrices/demo1/avg_attn_2_3.npy",
      "attention_matrices/demo1/avg_attn_2_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 3,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      3,
      4,
      4,
      4,
      5
    ],
    "target": [
      3,
      4,
      1,
      2,
      3,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "C->D\nD->E\nE->B\nE->C\nE->D\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_3.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_3_0.npy",
      "attention_matrices/demo1/avg_attn_3_1.npy",
      "attention_matrices/demo1/avg_attn_3_2.npy",
      "attention_matrices/demo1/avg_attn_3_3.npy",
      "attention_matrices/demo1/avg_attn_3_4.npy",
      "attention_matrices/demo1/avg_attn_3_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 4,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      3,
      5
    ],
    "target": [
      3,
      4,
      1,
      4,
      4,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->D\nB->E\nC->B\nC->E\nD->E\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_4.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_4_0.npy",
      "attention_matrices/demo1/avg_attn_4_1.npy",
      "attention_matrices/demo1/avg_attn_4_2.npy",
      "attention_matrices/demo1/avg_attn_4_3.npy",
      "attention_matrices/demo1/avg_attn_4_4.npy",
      "attention_matrices/demo1/avg_attn_4_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 5,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      1,
      1,
      2,
      3,
      5,
      5
    ],
    "target": [
      2,
      3,
      4,
      1,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->C\nB->D\nC->E\nD->B\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_5.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_5_0.npy",
      "attention_matrices/demo1/avg_attn_5_1.npy",
      "attention_matrices/demo1/avg_attn_5_2.npy",
      "attention_matrices/demo1/avg_attn_5_3.npy",
      "attention_matrices/demo1/avg_attn_5_4.npy",
      "attention_matrices/demo1/avg_attn_5_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 6,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      3,
      3,
      5,
      5,
      5
    ],
    "target": [
      4,
      2,
      3,
      3,
      0,
      4,
      0,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->E\nB->C\nB->D\nC->D\nD->A\nD->E\nF->A\nF->B\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_6.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_6_0.npy",
      "attention_matrices/demo1/avg_attn_6_1.npy",
      "attention_matrices/demo1/avg_attn_6_2.npy",
      "attention_matrices/demo1/avg_attn_6_3.npy",
      "attention_matrices/demo1/avg_attn_6_4.npy",
      "attention_matrices/demo1/avg_attn_6_5.npy",
      "attention_matrices/demo1/avg_attn_6_6.npy",
      "attention_matrices/demo1/avg_attn_6_7.npy",
      "attention_matrices/demo1/avg_attn_6_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 7,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      2,
      3,
      4,
      5,
      5
    ],
    "target": [
      3,
      0,
      3,
      4,
      1,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->D\nC->A\nC->D\nD->E\nE->B\nF->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_7.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_7_0.npy",
      "attention_matrices/demo1/avg_attn_7_1.npy",
      "attention_matrices/demo1/avg_attn_7_2.npy",
      "attention_matrices/demo1/avg_attn_7_3.npy",
      "attention_matrices/demo1/avg_attn_7_4.npy",
      "attention_matrices/demo1/avg_attn_7_5.npy",
      "attention_matrices/demo1/avg_attn_7_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 8,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      5
    ],
    "target": [
      4,
      2,
      4,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->E\nB->C\nC->E\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_8.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_8_0.npy",
      "attention_matrices/demo1/avg_attn_8_1.npy",
      "attention_matrices/demo1/avg_attn_8_2.npy",
      "attention_matrices/demo1/avg_attn_8_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 9,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      1,
      2,
      4,
      5
    ],
    "target": [
      2,
      3,
      4,
      0,
      1,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nA->E\nB->A\nC->B\nE->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_9.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_9_0.npy",
      "attention_matrices/demo1/avg_attn_9_1.npy",
      "attention_matrices/demo1/avg_attn_9_2.npy",
      "attention_matrices/demo1/avg_attn_9_3.npy",
      "attention_matrices/demo1/avg_attn_9_4.npy",
      "attention_matrices/demo1/avg_attn_9_5.npy",
      "attention_matrices/demo1/avg_attn_9_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 10,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      4,
      5
    ],
    "target": [
      1,
      3,
      0,
      3,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->B\nB->D\nC->A\nE->D\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_10.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_10_0.npy",
      "attention_matrices/demo1/avg_attn_10_1.npy",
      "attention_matrices/demo1/avg_attn_10_2.npy",
      "attention_matrices/demo1/avg_attn_10_3.npy",
      "attention_matrices/demo1/avg_attn_10_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 11,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      3,
      4,
      4
    ],
    "target": [
      3,
      1,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "C->D\nD->B\nE->C\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_11.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_11_0.npy",
      "attention_matrices/demo1/avg_attn_11_1.npy",
      "attention_matrices/demo1/avg_attn_11_2.npy",
      "attention_matrices/demo1/avg_attn_11_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 12,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      2,
      4,
      5,
      5,
      5
    ],
    "target": [
      3,
      0,
      0,
      0,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->D\nC->A\nE->A\nF->A\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_12.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_12_0.npy",
      "attention_matrices/demo1/avg_attn_12_1.npy",
      "attention_matrices/demo1/avg_attn_12_2.npy",
      "attention_matrices/demo1/avg_attn_12_3.npy",
      "attention_matrices/demo1/avg_attn_12_4.npy",
      "attention_matrices/demo1/avg_attn_12_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 13,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 3,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      3
    ],
    "target": [
      3,
      4,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->D\nA->E\nD->C\n",
    "averaged_attention_matrix_path": "averaged_id_13.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_13_0.npy",
      "attention_matrices/demo1/avg_attn_13_1.npy",
      "attention_matrices/demo1/avg_attn_13_2.npy"
    ],
    "num_averaged_samples": 3
  },
  {
    "graph_id": 14,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 10,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        1,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      2,
      2,
      4,
      5,
      5,
      5
    ],
    "target": [
      3,
      3,
      4,
      1,
      3,
      4,
      1,
      2,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->D\nB->D\nB->E\nC->B\nC->D\nC->E\nE->B\nF->C\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_14.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_14_0.npy",
      "attention_matrices/demo1/avg_attn_14_1.npy",
      "attention_matrices/demo1/avg_attn_14_2.npy",
      "attention_matrices/demo1/avg_attn_14_3.npy",
      "attention_matrices/demo1/avg_attn_14_4.npy",
      "attention_matrices/demo1/avg_attn_14_5.npy",
      "attention_matrices/demo1/avg_attn_14_6.npy",
      "attention_matrices/demo1/avg_attn_14_7.npy",
      "attention_matrices/demo1/avg_attn_14_8.npy",
      "attention_matrices/demo1/avg_attn_14_9.npy"
    ],
    "num_averaged_samples": 10
  },
  {
    "graph_id": 15,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      3,
      4
    ],
    "target": [
      2,
      4,
      4,
      3,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->E\nC->D\nD->A\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_15.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_15_0.npy",
      "attention_matrices/demo1/avg_attn_15_1.npy",
      "attention_matrices/demo1/avg_attn_15_2.npy",
      "attention_matrices/demo1/avg_attn_15_3.npy",
      "attention_matrices/demo1/avg_attn_15_4.npy",
      "attention_matrices/demo1/avg_attn_15_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 16,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      3,
      3,
      5
    ],
    "target": [
      1,
      0,
      3,
      4,
      1,
      4,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nB->A\nC->D\nC->E\nD->B\nD->E\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_16.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_16_0.npy",
      "attention_matrices/demo1/avg_attn_16_1.npy",
      "attention_matrices/demo1/avg_attn_16_2.npy",
      "attention_matrices/demo1/avg_attn_16_3.npy",
      "attention_matrices/demo1/avg_attn_16_4.npy",
      "attention_matrices/demo1/avg_attn_16_5.npy",
      "attention_matrices/demo1/avg_attn_16_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 17,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3,
      5
    ],
    "target": [
      3,
      4,
      0,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->D\nB->E\nC->A\nD->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_17.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_17_0.npy",
      "attention_matrices/demo1/avg_attn_17_1.npy",
      "attention_matrices/demo1/avg_attn_17_2.npy",
      "attention_matrices/demo1/avg_attn_17_3.npy",
      "attention_matrices/demo1/avg_attn_17_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 18,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ],
      [
        1,
        1,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      4,
      4,
      4,
      5,
      5,
      5
    ],
    "target": [
      2,
      3,
      0,
      0,
      2,
      3,
      0,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nB->A\nE->A\nE->C\nE->D\nF->A\nF->B\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_18.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_18_0.npy",
      "attention_matrices/demo1/avg_attn_18_1.npy",
      "attention_matrices/demo1/avg_attn_18_2.npy",
      "attention_matrices/demo1/avg_attn_18_3.npy",
      "attention_matrices/demo1/avg_attn_18_4.npy",
      "attention_matrices/demo1/avg_attn_18_5.npy",
      "attention_matrices/demo1/avg_attn_18_6.npy",
      "attention_matrices/demo1/avg_attn_18_7.npy",
      "attention_matrices/demo1/avg_attn_18_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 19,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      3,
      4,
      5
    ],
    "target": [
      1,
      4,
      0,
      0,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nA->E\nC->A\nD->A\nE->D\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_19.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_19_0.npy",
      "attention_matrices/demo1/avg_attn_19_1.npy",
      "attention_matrices/demo1/avg_attn_19_2.npy",
      "attention_matrices/demo1/avg_attn_19_3.npy",
      "attention_matrices/demo1/avg_attn_19_4.npy",
      "attention_matrices/demo1/avg_attn_19_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 20,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      3,
      4,
      5
    ],
    "target": [
      2,
      4,
      0,
      4,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nC->E\nD->A\nD->E\nE->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_20.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_20_0.npy",
      "attention_matrices/demo1/avg_attn_20_1.npy",
      "attention_matrices/demo1/avg_attn_20_2.npy",
      "attention_matrices/demo1/avg_attn_20_3.npy",
      "attention_matrices/demo1/avg_attn_20_4.npy",
      "attention_matrices/demo1/avg_attn_20_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 21,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      3,
      4,
      4
    ],
    "target": [
      0,
      0,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "C->A\nD->A\nE->B\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_21.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_21_0.npy",
      "attention_matrices/demo1/avg_attn_21_1.npy",
      "attention_matrices/demo1/avg_attn_21_2.npy",
      "attention_matrices/demo1/avg_attn_21_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 22,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      1,
      2,
      4,
      2,
      2,
      1,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->B\nB->C\nB->E\nD->C\nE->C\nF->B\nF->C\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_22.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_22_0.npy",
      "attention_matrices/demo1/avg_attn_22_1.npy",
      "attention_matrices/demo1/avg_attn_22_2.npy",
      "attention_matrices/demo1/avg_attn_22_3.npy",
      "attention_matrices/demo1/avg_attn_22_4.npy",
      "attention_matrices/demo1/avg_attn_22_5.npy",
      "attention_matrices/demo1/avg_attn_22_6.npy",
      "attention_matrices/demo1/avg_attn_22_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 23,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      4,
      2,
      0,
      1,
      4,
      1,
      0,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->E\nB->C\nC->A\nC->B\nD->E\nE->B\nF->A\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_23.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_23_0.npy",
      "attention_matrices/demo1/avg_attn_23_1.npy",
      "attention_matrices/demo1/avg_attn_23_2.npy",
      "attention_matrices/demo1/avg_attn_23_3.npy",
      "attention_matrices/demo1/avg_attn_23_4.npy",
      "attention_matrices/demo1/avg_attn_23_5.npy",
      "attention_matrices/demo1/avg_attn_23_6.npy",
      "attention_matrices/demo1/avg_attn_23_7.npy",
      "attention_matrices/demo1/avg_attn_23_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 24,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      1,
      2,
      2,
      4,
      5,
      5
    ],
    "target": [
      4,
      0,
      1,
      1,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->E\nC->A\nC->B\nE->B\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_24.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_24_0.npy",
      "attention_matrices/demo1/avg_attn_24_1.npy",
      "attention_matrices/demo1/avg_attn_24_2.npy",
      "attention_matrices/demo1/avg_attn_24_3.npy",
      "attention_matrices/demo1/avg_attn_24_4.npy",
      "attention_matrices/demo1/avg_attn_24_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 25,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      3,
      4,
      4,
      5,
      5
    ],
    "target": [
      0,
      4,
      2,
      0,
      3,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->A\nB->E\nD->C\nE->A\nE->D\nF->C\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_25.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_25_0.npy",
      "attention_matrices/demo1/avg_attn_25_1.npy",
      "attention_matrices/demo1/avg_attn_25_2.npy",
      "attention_matrices/demo1/avg_attn_25_3.npy",
      "attention_matrices/demo1/avg_attn_25_4.npy",
      "attention_matrices/demo1/avg_attn_25_5.npy",
      "attention_matrices/demo1/avg_attn_25_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 26,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      2,
      3,
      4,
      4
    ],
    "target": [
      2,
      1,
      3,
      1,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nC->B\nC->D\nD->B\nE->B\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_26.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_26_0.npy",
      "attention_matrices/demo1/avg_attn_26_1.npy",
      "attention_matrices/demo1/avg_attn_26_2.npy",
      "attention_matrices/demo1/avg_attn_26_3.npy",
      "attention_matrices/demo1/avg_attn_26_4.npy",
      "attention_matrices/demo1/avg_attn_26_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 27,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      0,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      3,
      0,
      1,
      1,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->D\nD->A\nE->B\nF->B\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_27.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_27_0.npy",
      "attention_matrices/demo1/avg_attn_27_1.npy",
      "attention_matrices/demo1/avg_attn_27_2.npy",
      "attention_matrices/demo1/avg_attn_27_3.npy",
      "attention_matrices/demo1/avg_attn_27_4.npy",
      "attention_matrices/demo1/avg_attn_27_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 28,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      5
    ],
    "target": [
      0,
      4,
      4,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "B->A\nC->E\nD->E\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_28.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_28_0.npy",
      "attention_matrices/demo1/avg_attn_28_1.npy",
      "attention_matrices/demo1/avg_attn_28_2.npy",
      "attention_matrices/demo1/avg_attn_28_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 29,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 3,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      5
    ],
    "target": [
      3,
      4,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->D\nC->E\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_29.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_29_0.npy",
      "attention_matrices/demo1/avg_attn_29_1.npy",
      "attention_matrices/demo1/avg_attn_29_2.npy"
    ],
    "num_averaged_samples": 3
  },
  {
    "graph_id": 30,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 10,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      2,
      3,
      3,
      5,
      5,
      5
    ],
    "target": [
      3,
      4,
      0,
      1,
      4,
      1,
      2,
      0,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->D\nB->E\nC->A\nC->B\nC->E\nD->B\nD->C\nF->A\nF->B\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_30.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_30_0.npy",
      "attention_matrices/demo1/avg_attn_30_1.npy",
      "attention_matrices/demo1/avg_attn_30_2.npy",
      "attention_matrices/demo1/avg_attn_30_3.npy",
      "attention_matrices/demo1/avg_attn_30_4.npy",
      "attention_matrices/demo1/avg_attn_30_5.npy",
      "attention_matrices/demo1/avg_attn_30_6.npy",
      "attention_matrices/demo1/avg_attn_30_7.npy",
      "attention_matrices/demo1/avg_attn_30_8.npy",
      "attention_matrices/demo1/avg_attn_30_9.npy"
    ],
    "num_averaged_samples": 10
  },
  {
    "graph_id": 31,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      4,
      5,
      5,
      5
    ],
    "target": [
      3,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "E->D\nF->B\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_31.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_31_0.npy",
      "attention_matrices/demo1/avg_attn_31_1.npy",
      "attention_matrices/demo1/avg_attn_31_2.npy",
      "attention_matrices/demo1/avg_attn_31_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 32,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      1,
      3,
      4,
      5,
      5
    ],
    "target": [
      0,
      2,
      0,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->A\nD->C\nE->A\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_32.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_32_0.npy",
      "attention_matrices/demo1/avg_attn_32_1.npy",
      "attention_matrices/demo1/avg_attn_32_2.npy",
      "attention_matrices/demo1/avg_attn_32_3.npy",
      "attention_matrices/demo1/avg_attn_32_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 33,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      3,
      3,
      4,
      5
    ],
    "target": [
      2,
      0,
      1,
      4,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nB->A\nD->B\nD->E\nE->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_33.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_33_0.npy",
      "attention_matrices/demo1/avg_attn_33_1.npy",
      "attention_matrices/demo1/avg_attn_33_2.npy",
      "attention_matrices/demo1/avg_attn_33_3.npy",
      "attention_matrices/demo1/avg_attn_33_4.npy",
      "attention_matrices/demo1/avg_attn_33_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 34,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      1,
      2,
      5,
      5
    ],
    "target": [
      3,
      0,
      2,
      3,
      3,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->D\nB->A\nB->C\nB->D\nC->D\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_34.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_34_0.npy",
      "attention_matrices/demo1/avg_attn_34_1.npy",
      "attention_matrices/demo1/avg_attn_34_2.npy",
      "attention_matrices/demo1/avg_attn_34_3.npy",
      "attention_matrices/demo1/avg_attn_34_4.npy",
      "attention_matrices/demo1/avg_attn_34_5.npy",
      "attention_matrices/demo1/avg_attn_34_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 35,
    "max_nodes": 6,
    "num_nodes": 3,
    "num_edges": 3,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2
    ],
    "target": [
      1,
      2,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nB->C\nC->B\n",
    "averaged_attention_matrix_path": "averaged_id_35.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_35_0.npy",
      "attention_matrices/demo1/avg_attn_35_1.npy",
      "attention_matrices/demo1/avg_attn_35_2.npy"
    ],
    "num_averaged_samples": 3
  },
  {
    "graph_id": 36,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      3,
      4
    ],
    "target": [
      2,
      4,
      1,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->C\nC->E\nD->B\nD->C\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_36.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_36_0.npy",
      "attention_matrices/demo1/avg_attn_36_1.npy",
      "attention_matrices/demo1/avg_attn_36_2.npy",
      "attention_matrices/demo1/avg_attn_36_3.npy",
      "attention_matrices/demo1/avg_attn_36_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 37,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 10,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      1,
      2,
      3,
      5,
      5,
      5
    ],
    "target": [
      3,
      4,
      0,
      2,
      3,
      3,
      2,
      0,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->D\nA->E\nB->A\nB->C\nB->D\nC->D\nD->C\nF->A\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_37.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_37_0.npy",
      "attention_matrices/demo1/avg_attn_37_1.npy",
      "attention_matrices/demo1/avg_attn_37_2.npy",
      "attention_matrices/demo1/avg_attn_37_3.npy",
      "attention_matrices/demo1/avg_attn_37_4.npy",
      "attention_matrices/demo1/avg_attn_37_5.npy",
      "attention_matrices/demo1/avg_attn_37_6.npy",
      "attention_matrices/demo1/avg_attn_37_7.npy",
      "attention_matrices/demo1/avg_attn_37_8.npy",
      "attention_matrices/demo1/avg_attn_37_9.npy"
    ],
    "num_averaged_samples": 10
  },
  {
    "graph_id": 38,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      2,
      4
    ],
    "target": [
      3,
      4,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->D\nB->E\nC->A\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_38.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_38_0.npy",
      "attention_matrices/demo1/avg_attn_38_1.npy",
      "attention_matrices/demo1/avg_attn_38_2.npy",
      "attention_matrices/demo1/avg_attn_38_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 39,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      3
    ],
    "target": [
      1,
      1,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nC->B\nD->A\nD->B\n",
    "averaged_attention_matrix_path": "averaged_id_39.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_39_0.npy",
      "attention_matrices/demo1/avg_attn_39_1.npy",
      "attention_matrices/demo1/avg_attn_39_2.npy",
      "attention_matrices/demo1/avg_attn_39_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 40,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      3,
      5
    ],
    "target": [
      2,
      4,
      0,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->A\nD->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_40.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_40_0.npy",
      "attention_matrices/demo1/avg_attn_40_1.npy",
      "attention_matrices/demo1/avg_attn_40_2.npy",
      "attention_matrices/demo1/avg_attn_40_3.npy",
      "attention_matrices/demo1/avg_attn_40_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 41,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      3,
      3,
      4,
      5
    ],
    "target": [
      2,
      3,
      3,
      1,
      2,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nC->D\nD->B\nD->C\nE->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_41.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_41_0.npy",
      "attention_matrices/demo1/avg_attn_41_1.npy",
      "attention_matrices/demo1/avg_attn_41_2.npy",
      "attention_matrices/demo1/avg_attn_41_3.npy",
      "attention_matrices/demo1/avg_attn_41_4.npy",
      "attention_matrices/demo1/avg_attn_41_5.npy",
      "attention_matrices/demo1/avg_attn_41_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 42,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      4,
      5
    ],
    "target": [
      2,
      3,
      4,
      1,
      2,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->C\nB->D\nB->E\nC->B\nE->C\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_42.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_42_0.npy",
      "attention_matrices/demo1/avg_attn_42_1.npy",
      "attention_matrices/demo1/avg_attn_42_2.npy",
      "attention_matrices/demo1/avg_attn_42_3.npy",
      "attention_matrices/demo1/avg_attn_42_4.npy",
      "attention_matrices/demo1/avg_attn_42_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 43,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      3,
      3,
      5
    ],
    "target": [
      1,
      3,
      1,
      4,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nD->B\nD->E\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_43.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_43_0.npy",
      "attention_matrices/demo1/avg_attn_43_1.npy",
      "attention_matrices/demo1/avg_attn_43_2.npy",
      "attention_matrices/demo1/avg_attn_43_3.npy",
      "attention_matrices/demo1/avg_attn_43_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 44,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      2,
      4,
      5
    ],
    "target": [
      1,
      3,
      2,
      0,
      4,
      2,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nB->C\nC->A\nC->E\nE->C\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_44.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_44_0.npy",
      "attention_matrices/demo1/avg_attn_44_1.npy",
      "attention_matrices/demo1/avg_attn_44_2.npy",
      "attention_matrices/demo1/avg_attn_44_3.npy",
      "attention_matrices/demo1/avg_attn_44_4.npy",
      "attention_matrices/demo1/avg_attn_44_5.npy",
      "attention_matrices/demo1/avg_attn_44_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 45,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      5,
      5
    ],
    "target": [
      1,
      2,
      4,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nB->C\nB->E\nC->B\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_45.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_45_0.npy",
      "attention_matrices/demo1/avg_attn_45_1.npy",
      "attention_matrices/demo1/avg_attn_45_2.npy",
      "attention_matrices/demo1/avg_attn_45_3.npy",
      "attention_matrices/demo1/avg_attn_45_4.npy",
      "attention_matrices/demo1/avg_attn_45_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 46,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 3,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      4,
      5
    ],
    "target": [
      1,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "C->B\nE->B\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_46.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_46_0.npy",
      "attention_matrices/demo1/avg_attn_46_1.npy",
      "attention_matrices/demo1/avg_attn_46_2.npy"
    ],
    "num_averaged_samples": 3
  },
  {
    "graph_id": 47,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      4,
      5
    ],
    "target": [
      2,
      3,
      1,
      2,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nC->B\nE->C\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_47.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_47_0.npy",
      "attention_matrices/demo1/avg_attn_47_1.npy",
      "attention_matrices/demo1/avg_attn_47_2.npy",
      "attention_matrices/demo1/avg_attn_47_3.npy",
      "attention_matrices/demo1/avg_attn_47_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 48,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      3,
      4,
      4,
      5,
      5,
      5
    ],
    "target": [
      1,
      3,
      1,
      0,
      2,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nB->D\nD->B\nE->A\nE->C\nF->B\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_48.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_48_0.npy",
      "attention_matrices/demo1/avg_attn_48_1.npy",
      "attention_matrices/demo1/avg_attn_48_2.npy",
      "attention_matrices/demo1/avg_attn_48_3.npy",
      "attention_matrices/demo1/avg_attn_48_4.npy",
      "attention_matrices/demo1/avg_attn_48_5.npy",
      "attention_matrices/demo1/avg_attn_48_6.npy",
      "attention_matrices/demo1/avg_attn_48_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 49,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      3,
      4
    ],
    "target": [
      2,
      4,
      3,
      4,
      4,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nB->E\nC->D\nC->E\nD->E\nE->A\n",
    "averaged_attention_matrix_path": "averaged_id_49.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_49_0.npy",
      "attention_matrices/demo1/avg_attn_49_1.npy",
      "attention_matrices/demo1/avg_attn_49_2.npy",
      "attention_matrices/demo1/avg_attn_49_3.npy",
      "attention_matrices/demo1/avg_attn_49_4.npy",
      "attention_matrices/demo1/avg_attn_49_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 50,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3
    ],
    "target": [
      1,
      0,
      0,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nB->A\nC->A\nD->E\n",
    "averaged_attention_matrix_path": "averaged_id_50.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_50_0.npy",
      "attention_matrices/demo1/avg_attn_50_1.npy",
      "attention_matrices/demo1/avg_attn_50_2.npy",
      "attention_matrices/demo1/avg_attn_50_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 51,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      2,
      3,
      3,
      3,
      4,
      4,
      4,
      5
    ],
    "target": [
      1,
      0,
      2,
      4,
      0,
      2,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "C->B\nD->A\nD->C\nD->E\nE->A\nE->C\nE->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_51.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_51_0.npy",
      "attention_matrices/demo1/avg_attn_51_1.npy",
      "attention_matrices/demo1/avg_attn_51_2.npy",
      "attention_matrices/demo1/avg_attn_51_3.npy",
      "attention_matrices/demo1/avg_attn_51_4.npy",
      "attention_matrices/demo1/avg_attn_51_5.npy",
      "attention_matrices/demo1/avg_attn_51_6.npy",
      "attention_matrices/demo1/avg_attn_51_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 52,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 13,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      1,
      2,
      3,
      3,
      4,
      4,
      5,
      5,
      5
    ],
    "target": [
      1,
      2,
      0,
      2,
      3,
      3,
      0,
      2,
      0,
      2,
      0,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nA->C\nB->A\nB->C\nB->D\nC->D\nD->A\nD->C\nE->A\nE->C\nF->A\nF->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_52.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_52_0.npy",
      "attention_matrices/demo1/avg_attn_52_1.npy",
      "attention_matrices/demo1/avg_attn_52_2.npy",
      "attention_matrices/demo1/avg_attn_52_3.npy",
      "attention_matrices/demo1/avg_attn_52_4.npy",
      "attention_matrices/demo1/avg_attn_52_5.npy",
      "attention_matrices/demo1/avg_attn_52_6.npy",
      "attention_matrices/demo1/avg_attn_52_7.npy",
      "attention_matrices/demo1/avg_attn_52_8.npy",
      "attention_matrices/demo1/avg_attn_52_9.npy",
      "attention_matrices/demo1/avg_attn_52_10.npy",
      "attention_matrices/demo1/avg_attn_52_11.npy",
      "attention_matrices/demo1/avg_attn_52_12.npy"
    ],
    "num_averaged_samples": 13
  },
  {
    "graph_id": 53,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      2,
      3,
      5
    ],
    "target": [
      0,
      0,
      3,
      4,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "B->A\nC->A\nC->D\nD->E\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_53.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_53_0.npy",
      "attention_matrices/demo1/avg_attn_53_1.npy",
      "attention_matrices/demo1/avg_attn_53_2.npy",
      "attention_matrices/demo1/avg_attn_53_3.npy",
      "attention_matrices/demo1/avg_attn_53_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 54,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      4,
      5,
      5
    ],
    "target": [
      2,
      0,
      3,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nB->A\nE->D\nF->B\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_54.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_54_0.npy",
      "attention_matrices/demo1/avg_attn_54_1.npy",
      "attention_matrices/demo1/avg_attn_54_2.npy",
      "attention_matrices/demo1/avg_attn_54_3.npy",
      "attention_matrices/demo1/avg_attn_54_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 55,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      1,
      3
    ],
    "target": [
      2,
      0,
      2,
      4,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nB->A\nB->C\nB->E\nD->C\n",
    "averaged_attention_matrix_path": "averaged_id_55.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_55_0.npy",
      "attention_matrices/demo1/avg_attn_55_1.npy",
      "attention_matrices/demo1/avg_attn_55_2.npy",
      "attention_matrices/demo1/avg_attn_55_3.npy",
      "attention_matrices/demo1/avg_attn_55_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 56,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3,
      5
    ],
    "target": [
      1,
      3,
      4,
      4,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->B\nB->D\nC->E\nD->E\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_56.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_56_0.npy",
      "attention_matrices/demo1/avg_attn_56_1.npy",
      "attention_matrices/demo1/avg_attn_56_2.npy",
      "attention_matrices/demo1/avg_attn_56_3.npy",
      "attention_matrices/demo1/avg_attn_56_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 57,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        1,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      1,
      3,
      5
    ],
    "target": [
      3,
      4,
      0,
      3,
      4,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->D\nA->E\nB->A\nB->D\nB->E\nD->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_57.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_57_0.npy",
      "attention_matrices/demo1/avg_attn_57_1.npy",
      "attention_matrices/demo1/avg_attn_57_2.npy",
      "attention_matrices/demo1/avg_attn_57_3.npy",
      "attention_matrices/demo1/avg_attn_57_4.npy",
      "attention_matrices/demo1/avg_attn_57_5.npy",
      "attention_matrices/demo1/avg_attn_57_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 58,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      3,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      1,
      2,
      1,
      4,
      0,
      0,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nB->C\nD->B\nD->E\nE->A\nF->A\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_58.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_58_0.npy",
      "attention_matrices/demo1/avg_attn_58_1.npy",
      "attention_matrices/demo1/avg_attn_58_2.npy",
      "attention_matrices/demo1/avg_attn_58_3.npy",
      "attention_matrices/demo1/avg_attn_58_4.npy",
      "attention_matrices/demo1/avg_attn_58_5.npy",
      "attention_matrices/demo1/avg_attn_58_6.npy",
      "attention_matrices/demo1/avg_attn_58_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 59,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 11,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        1,
        1,
        1,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      0,
      1,
      1,
      2,
      2,
      3,
      4,
      4
    ],
    "target": [
      1,
      2,
      3,
      4,
      0,
      3,
      0,
      4,
      2,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->B\nA->C\nA->D\nA->E\nB->A\nB->D\nC->A\nC->E\nD->C\nE->C\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_59.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_59_0.npy",
      "attention_matrices/demo1/avg_attn_59_1.npy",
      "attention_matrices/demo1/avg_attn_59_2.npy",
      "attention_matrices/demo1/avg_attn_59_3.npy",
      "attention_matrices/demo1/avg_attn_59_4.npy",
      "attention_matrices/demo1/avg_attn_59_5.npy",
      "attention_matrices/demo1/avg_attn_59_6.npy",
      "attention_matrices/demo1/avg_attn_59_7.npy",
      "attention_matrices/demo1/avg_attn_59_8.npy",
      "attention_matrices/demo1/avg_attn_59_9.npy",
      "attention_matrices/demo1/avg_attn_59_10.npy"
    ],
    "num_averaged_samples": 11
  },
  {
    "graph_id": 60,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      2,
      2,
      4,
      4,
      5
    ],
    "target": [
      4,
      1,
      4,
      1,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->E\nC->B\nC->E\nE->B\nE->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_60.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_60_0.npy",
      "attention_matrices/demo1/avg_attn_60_1.npy",
      "attention_matrices/demo1/avg_attn_60_2.npy",
      "attention_matrices/demo1/avg_attn_60_3.npy",
      "attention_matrices/demo1/avg_attn_60_4.npy",
      "attention_matrices/demo1/avg_attn_60_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 61,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      5
    ],
    "target": [
      3,
      1,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->D\nC->B\nD->B\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_61.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_61_0.npy",
      "attention_matrices/demo1/avg_attn_61_1.npy",
      "attention_matrices/demo1/avg_attn_61_2.npy",
      "attention_matrices/demo1/avg_attn_61_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 62,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      2,
      2,
      4,
      5
    ],
    "target": [
      1,
      3,
      0,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "C->B\nC->D\nE->A\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_62.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_62_0.npy",
      "attention_matrices/demo1/avg_attn_62_1.npy",
      "attention_matrices/demo1/avg_attn_62_2.npy",
      "attention_matrices/demo1/avg_attn_62_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 63,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      3,
      3,
      4,
      5
    ],
    "target": [
      1,
      2,
      4,
      2,
      4,
      0,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->B\nB->C\nB->E\nD->C\nD->E\nE->A\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_63.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_63_0.npy",
      "attention_matrices/demo1/avg_attn_63_1.npy",
      "attention_matrices/demo1/avg_attn_63_2.npy",
      "attention_matrices/demo1/avg_attn_63_3.npy",
      "attention_matrices/demo1/avg_attn_63_4.npy",
      "attention_matrices/demo1/avg_attn_63_5.npy",
      "attention_matrices/demo1/avg_attn_63_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 64,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      3,
      4,
      5
    ],
    "target": [
      3,
      4,
      2,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->D\nA->E\nD->C\nE->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_64.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_64_0.npy",
      "attention_matrices/demo1/avg_attn_64_1.npy",
      "attention_matrices/demo1/avg_attn_64_2.npy",
      "attention_matrices/demo1/avg_attn_64_3.npy",
      "attention_matrices/demo1/avg_attn_64_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 65,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      3,
      4,
      5,
      5
    ],
    "target": [
      2,
      4,
      0,
      0,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nB->E\nD->A\nE->A\nF->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_65.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_65_0.npy",
      "attention_matrices/demo1/avg_attn_65_1.npy",
      "attention_matrices/demo1/avg_attn_65_2.npy",
      "attention_matrices/demo1/avg_attn_65_3.npy",
      "attention_matrices/demo1/avg_attn_65_4.npy",
      "attention_matrices/demo1/avg_attn_65_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 66,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      2,
      3,
      5,
      5
    ],
    "target": [
      1,
      4,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "C->B\nD->E\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_66.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_66_0.npy",
      "attention_matrices/demo1/avg_attn_66_1.npy",
      "attention_matrices/demo1/avg_attn_66_2.npy",
      "attention_matrices/demo1/avg_attn_66_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 67,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      1,
      2,
      4,
      5
    ],
    "target": [
      2,
      4,
      0,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->C\nB->E\nC->A\nE->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_67.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_67_0.npy",
      "attention_matrices/demo1/avg_attn_67_1.npy",
      "attention_matrices/demo1/avg_attn_67_2.npy",
      "attention_matrices/demo1/avg_attn_67_3.npy",
      "attention_matrices/demo1/avg_attn_67_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 68,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      4,
      4,
      5
    ],
    "target": [
      3,
      2,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "B->D\nE->C\nE->D\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_68.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_68_0.npy",
      "attention_matrices/demo1/avg_attn_68_1.npy",
      "attention_matrices/demo1/avg_attn_68_2.npy",
      "attention_matrices/demo1/avg_attn_68_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 69,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      1,
      2,
      3,
      4,
      4,
      5
    ],
    "target": [
      1,
      2,
      3,
      2,
      4,
      4,
      0,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nA->C\nA->D\nB->C\nC->E\nD->E\nE->A\nE->D\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_69.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_69_0.npy",
      "attention_matrices/demo1/avg_attn_69_1.npy",
      "attention_matrices/demo1/avg_attn_69_2.npy",
      "attention_matrices/demo1/avg_attn_69_3.npy",
      "attention_matrices/demo1/avg_attn_69_4.npy",
      "attention_matrices/demo1/avg_attn_69_5.npy",
      "attention_matrices/demo1/avg_attn_69_6.npy",
      "attention_matrices/demo1/avg_attn_69_7.npy",
      "attention_matrices/demo1/avg_attn_69_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 70,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      3,
      4,
      4,
      5,
      5
    ],
    "target": [
      3,
      2,
      1,
      2,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->D\nD->C\nE->B\nE->C\nF->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_70.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_70_0.npy",
      "attention_matrices/demo1/avg_attn_70_1.npy",
      "attention_matrices/demo1/avg_attn_70_2.npy",
      "attention_matrices/demo1/avg_attn_70_3.npy",
      "attention_matrices/demo1/avg_attn_70_4.npy",
      "attention_matrices/demo1/avg_attn_70_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 71,
    "max_nodes": 6,
    "num_nodes": 3,
    "num_edges": 2,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1
    ],
    "target": [
      2,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nB->A\n",
    "averaged_attention_matrix_path": "averaged_id_71.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_71_0.npy",
      "attention_matrices/demo1/avg_attn_71_1.npy"
    ],
    "num_averaged_samples": 2
  },
  {
    "graph_id": 72,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      4,
      4
    ],
    "target": [
      2,
      3,
      3,
      4,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nB->D\nB->E\nE->B\nE->C\n",
    "averaged_attention_matrix_path": "averaged_id_72.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_72_0.npy",
      "attention_matrices/demo1/avg_attn_72_1.npy",
      "attention_matrices/demo1/avg_attn_72_2.npy",
      "attention_matrices/demo1/avg_attn_72_3.npy",
      "attention_matrices/demo1/avg_attn_72_4.npy",
      "attention_matrices/demo1/avg_attn_72_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 73,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      3,
      5,
      5
    ],
    "target": [
      4,
      3,
      0,
      1,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->E\nC->D\nD->A\nD->B\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_73.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_73_0.npy",
      "attention_matrices/demo1/avg_attn_73_1.npy",
      "attention_matrices/demo1/avg_attn_73_2.npy",
      "attention_matrices/demo1/avg_attn_73_3.npy",
      "attention_matrices/demo1/avg_attn_73_4.npy",
      "attention_matrices/demo1/avg_attn_73_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 74,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      3,
      3,
      3,
      4,
      5,
      5
    ],
    "target": [
      2,
      3,
      0,
      2,
      4,
      3,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->C\nB->D\nD->A\nD->C\nD->E\nE->D\nF->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_74.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_74_0.npy",
      "attention_matrices/demo1/avg_attn_74_1.npy",
      "attention_matrices/demo1/avg_attn_74_2.npy",
      "attention_matrices/demo1/avg_attn_74_3.npy",
      "attention_matrices/demo1/avg_attn_74_4.npy",
      "attention_matrices/demo1/avg_attn_74_5.npy",
      "attention_matrices/demo1/avg_attn_74_6.npy",
      "attention_matrices/demo1/avg_attn_74_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 75,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      4,
      4,
      5
    ],
    "target": [
      4,
      0,
      1,
      1,
      2,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->E\nC->A\nD->B\nE->B\nE->C\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_75.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_75_0.npy",
      "attention_matrices/demo1/avg_attn_75_1.npy",
      "attention_matrices/demo1/avg_attn_75_2.npy",
      "attention_matrices/demo1/avg_attn_75_3.npy",
      "attention_matrices/demo1/avg_attn_75_4.npy",
      "attention_matrices/demo1/avg_attn_75_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 76,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      2,
      4,
      4,
      5
    ],
    "target": [
      4,
      3,
      4,
      0,
      3,
      0,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->E\nB->D\nB->E\nC->A\nC->D\nE->A\nE->C\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_76.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_76_0.npy",
      "attention_matrices/demo1/avg_attn_76_1.npy",
      "attention_matrices/demo1/avg_attn_76_2.npy",
      "attention_matrices/demo1/avg_attn_76_3.npy",
      "attention_matrices/demo1/avg_attn_76_4.npy",
      "attention_matrices/demo1/avg_attn_76_5.npy",
      "attention_matrices/demo1/avg_attn_76_6.npy",
      "attention_matrices/demo1/avg_attn_76_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 77,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3,
      4,
      4,
      5,
      5,
      5
    ],
    "target": [
      1,
      4,
      1,
      4,
      2,
      3,
      0,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->B\nB->E\nC->B\nD->E\nE->C\nE->D\nF->A\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_77.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_77_0.npy",
      "attention_matrices/demo1/avg_attn_77_1.npy",
      "attention_matrices/demo1/avg_attn_77_2.npy",
      "attention_matrices/demo1/avg_attn_77_3.npy",
      "attention_matrices/demo1/avg_attn_77_4.npy",
      "attention_matrices/demo1/avg_attn_77_5.npy",
      "attention_matrices/demo1/avg_attn_77_6.npy",
      "attention_matrices/demo1/avg_attn_77_7.npy",
      "attention_matrices/demo1/avg_attn_77_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 78,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      4,
      5,
      5
    ],
    "target": [
      3,
      4,
      0,
      1,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->D\nA->E\nB->A\nE->B\nF->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_78.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_78_0.npy",
      "attention_matrices/demo1/avg_attn_78_1.npy",
      "attention_matrices/demo1/avg_attn_78_2.npy",
      "attention_matrices/demo1/avg_attn_78_3.npy",
      "attention_matrices/demo1/avg_attn_78_4.npy",
      "attention_matrices/demo1/avg_attn_78_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 79,
    "max_nodes": 6,
    "num_nodes": 3,
    "num_edges": 2,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      3,
      4
    ],
    "target": [
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "D->B\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_79.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_79_0.npy",
      "attention_matrices/demo1/avg_attn_79_1.npy"
    ],
    "num_averaged_samples": 2
  },
  {
    "graph_id": 80,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      3,
      4,
      4,
      4
    ],
    "target": [
      2,
      3,
      0,
      1,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nD->A\nE->B\nE->C\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_80.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_80_0.npy",
      "attention_matrices/demo1/avg_attn_80_1.npy",
      "attention_matrices/demo1/avg_attn_80_2.npy",
      "attention_matrices/demo1/avg_attn_80_3.npy",
      "attention_matrices/demo1/avg_attn_80_4.npy",
      "attention_matrices/demo1/avg_attn_80_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 81,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      1,
      1,
      3,
      4,
      4,
      4,
      5,
      5
    ],
    "target": [
      0,
      2,
      3,
      0,
      0,
      1,
      3,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->A\nB->C\nB->D\nD->A\nE->A\nE->B\nE->D\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_81.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_81_0.npy",
      "attention_matrices/demo1/avg_attn_81_1.npy",
      "attention_matrices/demo1/avg_attn_81_2.npy",
      "attention_matrices/demo1/avg_attn_81_3.npy",
      "attention_matrices/demo1/avg_attn_81_4.npy",
      "attention_matrices/demo1/avg_attn_81_5.npy",
      "attention_matrices/demo1/avg_attn_81_6.npy",
      "attention_matrices/demo1/avg_attn_81_7.npy",
      "attention_matrices/demo1/avg_attn_81_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 82,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      3,
      4,
      5,
      5
    ],
    "target": [
      1,
      0,
      1,
      3,
      0,
      0,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->B\nB->A\nC->B\nC->D\nD->A\nE->A\nF->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_82.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_82_0.npy",
      "attention_matrices/demo1/avg_attn_82_1.npy",
      "attention_matrices/demo1/avg_attn_82_2.npy",
      "attention_matrices/demo1/avg_attn_82_3.npy",
      "attention_matrices/demo1/avg_attn_82_4.npy",
      "attention_matrices/demo1/avg_attn_82_5.npy",
      "attention_matrices/demo1/avg_attn_82_6.npy",
      "attention_matrices/demo1/avg_attn_82_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 83,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      4
    ],
    "target": [
      3,
      4,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->D\nC->E\nD->B\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_83.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_83_0.npy",
      "attention_matrices/demo1/avg_attn_83_1.npy",
      "attention_matrices/demo1/avg_attn_83_2.npy",
      "attention_matrices/demo1/avg_attn_83_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 84,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      4,
      4
    ],
    "target": [
      4,
      2,
      3,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->E\nB->C\nC->D\nE->B\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_84.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_84_0.npy",
      "attention_matrices/demo1/avg_attn_84_1.npy",
      "attention_matrices/demo1/avg_attn_84_2.npy",
      "attention_matrices/demo1/avg_attn_84_3.npy",
      "attention_matrices/demo1/avg_attn_84_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 85,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      4,
      4
    ],
    "target": [
      4,
      0,
      0,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->E\nC->A\nD->A\nE->A\nE->B\n",
    "averaged_attention_matrix_path": "averaged_id_85.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_85_0.npy",
      "attention_matrices/demo1/avg_attn_85_1.npy",
      "attention_matrices/demo1/avg_attn_85_2.npy",
      "attention_matrices/demo1/avg_attn_85_3.npy",
      "attention_matrices/demo1/avg_attn_85_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 86,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      3,
      5
    ],
    "target": [
      2,
      4,
      0,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->A\nD->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_86.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_86_0.npy",
      "attention_matrices/demo1/avg_attn_86_1.npy",
      "attention_matrices/demo1/avg_attn_86_2.npy",
      "attention_matrices/demo1/avg_attn_86_3.npy",
      "attention_matrices/demo1/avg_attn_86_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 87,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      2,
      4,
      4,
      5
    ],
    "target": [
      2,
      3,
      1,
      4,
      1,
      2,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nC->B\nC->E\nE->B\nE->C\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_87.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_87_0.npy",
      "attention_matrices/demo1/avg_attn_87_1.npy",
      "attention_matrices/demo1/avg_attn_87_2.npy",
      "attention_matrices/demo1/avg_attn_87_3.npy",
      "attention_matrices/demo1/avg_attn_87_4.npy",
      "attention_matrices/demo1/avg_attn_87_5.npy",
      "attention_matrices/demo1/avg_attn_87_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 88,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      1,
      2,
      5,
      5
    ],
    "target": [
      1,
      3,
      4,
      4,
      1,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nA->E\nB->E\nC->B\nF->A\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_88.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_88_0.npy",
      "attention_matrices/demo1/avg_attn_88_1.npy",
      "attention_matrices/demo1/avg_attn_88_2.npy",
      "attention_matrices/demo1/avg_attn_88_3.npy",
      "attention_matrices/demo1/avg_attn_88_4.npy",
      "attention_matrices/demo1/avg_attn_88_5.npy",
      "attention_matrices/demo1/avg_attn_88_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 89,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      4,
      4,
      5
    ],
    "target": [
      2,
      1,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->C\nE->B\nE->D\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_89.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_89_0.npy",
      "attention_matrices/demo1/avg_attn_89_1.npy",
      "attention_matrices/demo1/avg_attn_89_2.npy",
      "attention_matrices/demo1/avg_attn_89_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 90,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      2,
      3
    ],
    "target": [
      1,
      0,
      2,
      0,
      4,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->B\nB->A\nB->C\nC->A\nC->E\nD->A\n",
    "averaged_attention_matrix_path": "averaged_id_90.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_90_0.npy",
      "attention_matrices/demo1/avg_attn_90_1.npy",
      "attention_matrices/demo1/avg_attn_90_2.npy",
      "attention_matrices/demo1/avg_attn_90_3.npy",
      "attention_matrices/demo1/avg_attn_90_4.npy",
      "attention_matrices/demo1/avg_attn_90_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 91,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      3,
      3,
      4,
      4,
      5
    ],
    "target": [
      2,
      1,
      2,
      1,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "B->C\nD->B\nD->C\nE->B\nE->D\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_91.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_91_0.npy",
      "attention_matrices/demo1/avg_attn_91_1.npy",
      "attention_matrices/demo1/avg_attn_91_2.npy",
      "attention_matrices/demo1/avg_attn_91_3.npy",
      "attention_matrices/demo1/avg_attn_91_4.npy",
      "attention_matrices/demo1/avg_attn_91_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 92,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      3,
      5,
      5
    ],
    "target": [
      2,
      3,
      4,
      4,
      2,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nB->E\nC->E\nD->C\nF->A\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_92.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_92_0.npy",
      "attention_matrices/demo1/avg_attn_92_1.npy",
      "attention_matrices/demo1/avg_attn_92_2.npy",
      "attention_matrices/demo1/avg_attn_92_3.npy",
      "attention_matrices/demo1/avg_attn_92_4.npy",
      "attention_matrices/demo1/avg_attn_92_5.npy",
      "attention_matrices/demo1/avg_attn_92_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 93,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      2,
      3,
      4,
      4,
      5,
      5
    ],
    "target": [
      0,
      0,
      4,
      4,
      0,
      1,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->A\nC->A\nC->E\nD->E\nE->A\nE->B\nF->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_93.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_93_0.npy",
      "attention_matrices/demo1/avg_attn_93_1.npy",
      "attention_matrices/demo1/avg_attn_93_2.npy",
      "attention_matrices/demo1/avg_attn_93_3.npy",
      "attention_matrices/demo1/avg_attn_93_4.npy",
      "attention_matrices/demo1/avg_attn_93_5.npy",
      "attention_matrices/demo1/avg_attn_93_6.npy",
      "attention_matrices/demo1/avg_attn_93_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 94,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      5
    ],
    "target": [
      2,
      2,
      4,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nB->C\nB->E\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_94.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_94_0.npy",
      "attention_matrices/demo1/avg_attn_94_1.npy",
      "attention_matrices/demo1/avg_attn_94_2.npy",
      "attention_matrices/demo1/avg_attn_94_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 95,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 3,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      3,
      4
    ],
    "target": [
      3,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "C->D\nD->A\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_95.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_95_0.npy",
      "attention_matrices/demo1/avg_attn_95_1.npy",
      "attention_matrices/demo1/avg_attn_95_2.npy"
    ],
    "num_averaged_samples": 3
  },
  {
    "graph_id": 96,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      3,
      5
    ],
    "target": [
      1,
      4,
      2,
      0,
      0,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->B\nA->E\nB->C\nC->A\nD->A\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_96.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_96_0.npy",
      "attention_matrices/demo1/avg_attn_96_1.npy",
      "attention_matrices/demo1/avg_attn_96_2.npy",
      "attention_matrices/demo1/avg_attn_96_3.npy",
      "attention_matrices/demo1/avg_attn_96_4.npy",
      "attention_matrices/demo1/avg_attn_96_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 97,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      2,
      4,
      5
    ],
    "target": [
      3,
      4,
      0,
      2,
      1,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->D\nA->E\nB->A\nB->C\nC->B\nE->D\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_97.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_97_0.npy",
      "attention_matrices/demo1/avg_attn_97_1.npy",
      "attention_matrices/demo1/avg_attn_97_2.npy",
      "attention_matrices/demo1/avg_attn_97_3.npy",
      "attention_matrices/demo1/avg_attn_97_4.npy",
      "attention_matrices/demo1/avg_attn_97_5.npy",
      "attention_matrices/demo1/avg_attn_97_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 98,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      4,
      5,
      5
    ],
    "target": [
      4,
      0,
      0,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->E\nC->A\nE->A\nF->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_98.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_98_0.npy",
      "attention_matrices/demo1/avg_attn_98_1.npy",
      "attention_matrices/demo1/avg_attn_98_2.npy",
      "attention_matrices/demo1/avg_attn_98_3.npy",
      "attention_matrices/demo1/avg_attn_98_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 99,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      3,
      4,
      5
    ],
    "target": [
      2,
      4,
      2,
      2,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nD->C\nE->C\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_99.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_99_0.npy",
      "attention_matrices/demo1/avg_attn_99_1.npy",
      "attention_matrices/demo1/avg_attn_99_2.npy",
      "attention_matrices/demo1/avg_attn_99_3.npy",
      "attention_matrices/demo1/avg_attn_99_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 100,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      3,
      3,
      4,
      5
    ],
    "target": [
      0,
      2,
      2,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "D->A\nD->C\nE->C\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_100.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_100_0.npy",
      "attention_matrices/demo1/avg_attn_100_1.npy",
      "attention_matrices/demo1/avg_attn_100_2.npy",
      "attention_matrices/demo1/avg_attn_100_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 101,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      4,
      4,
      5
    ],
    "target": [
      3,
      3,
      0,
      2,
      3,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->D\nC->D\nD->A\nE->C\nE->D\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_101.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_101_0.npy",
      "attention_matrices/demo1/avg_attn_101_1.npy",
      "attention_matrices/demo1/avg_attn_101_2.npy",
      "attention_matrices/demo1/avg_attn_101_3.npy",
      "attention_matrices/demo1/avg_attn_101_4.npy",
      "attention_matrices/demo1/avg_attn_101_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 102,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      3,
      3,
      4,
      5
    ],
    "target": [
      2,
      3,
      0,
      2,
      4,
      1,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nB->A\nD->C\nD->E\nE->B\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_102.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_102_0.npy",
      "attention_matrices/demo1/avg_attn_102_1.npy",
      "attention_matrices/demo1/avg_attn_102_2.npy",
      "attention_matrices/demo1/avg_attn_102_3.npy",
      "attention_matrices/demo1/avg_attn_102_4.npy",
      "attention_matrices/demo1/avg_attn_102_5.npy",
      "attention_matrices/demo1/avg_attn_102_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 103,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      1,
      3,
      3,
      5
    ],
    "target": [
      4,
      2,
      3,
      4,
      1,
      2,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->E\nB->C\nB->D\nB->E\nD->B\nD->C\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_103.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_103_0.npy",
      "attention_matrices/demo1/avg_attn_103_1.npy",
      "attention_matrices/demo1/avg_attn_103_2.npy",
      "attention_matrices/demo1/avg_attn_103_3.npy",
      "attention_matrices/demo1/avg_attn_103_4.npy",
      "attention_matrices/demo1/avg_attn_103_5.npy",
      "attention_matrices/demo1/avg_attn_103_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 104,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      2,
      2,
      3,
      5,
      5
    ],
    "target": [
      2,
      3,
      0,
      3,
      0,
      4,
      2,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nB->A\nB->D\nC->A\nC->E\nD->C\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_104.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_104_0.npy",
      "attention_matrices/demo1/avg_attn_104_1.npy",
      "attention_matrices/demo1/avg_attn_104_2.npy",
      "attention_matrices/demo1/avg_attn_104_3.npy",
      "attention_matrices/demo1/avg_attn_104_4.npy",
      "attention_matrices/demo1/avg_attn_104_5.npy",
      "attention_matrices/demo1/avg_attn_104_6.npy",
      "attention_matrices/demo1/avg_attn_104_7.npy",
      "attention_matrices/demo1/avg_attn_104_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 105,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      3,
      4,
      4,
      4,
      5
    ],
    "target": [
      0,
      3,
      0,
      0,
      2,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "B->A\nB->D\nD->A\nE->A\nE->C\nE->D\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_105.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_105_0.npy",
      "attention_matrices/demo1/avg_attn_105_1.npy",
      "attention_matrices/demo1/avg_attn_105_2.npy",
      "attention_matrices/demo1/avg_attn_105_3.npy",
      "attention_matrices/demo1/avg_attn_105_4.npy",
      "attention_matrices/demo1/avg_attn_105_5.npy",
      "attention_matrices/demo1/avg_attn_105_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 106,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      3,
      4,
      4,
      4,
      5
    ],
    "target": [
      2,
      3,
      2,
      1,
      2,
      1,
      2,
      3,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nB->C\nC->B\nD->C\nE->B\nE->C\nE->D\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_106.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_106_0.npy",
      "attention_matrices/demo1/avg_attn_106_1.npy",
      "attention_matrices/demo1/avg_attn_106_2.npy",
      "attention_matrices/demo1/avg_attn_106_3.npy",
      "attention_matrices/demo1/avg_attn_106_4.npy",
      "attention_matrices/demo1/avg_attn_106_5.npy",
      "attention_matrices/demo1/avg_attn_106_6.npy",
      "attention_matrices/demo1/avg_attn_106_7.npy",
      "attention_matrices/demo1/avg_attn_106_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 107,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      4,
      4,
      5
    ],
    "target": [
      2,
      4,
      1,
      2,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->C\nB->E\nE->B\nE->C\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_107.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_107_0.npy",
      "attention_matrices/demo1/avg_attn_107_1.npy",
      "attention_matrices/demo1/avg_attn_107_2.npy",
      "attention_matrices/demo1/avg_attn_107_3.npy",
      "attention_matrices/demo1/avg_attn_107_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 108,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      2,
      5
    ],
    "target": [
      1,
      3,
      0,
      4,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nC->A\nC->E\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_108.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_108_0.npy",
      "attention_matrices/demo1/avg_attn_108_1.npy",
      "attention_matrices/demo1/avg_attn_108_2.npy",
      "attention_matrices/demo1/avg_attn_108_3.npy",
      "attention_matrices/demo1/avg_attn_108_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 109,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3,
      5,
      5,
      5
    ],
    "target": [
      2,
      3,
      0,
      4,
      0,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nB->D\nC->A\nD->E\nF->A\nF->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_109.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_109_0.npy",
      "attention_matrices/demo1/avg_attn_109_1.npy",
      "attention_matrices/demo1/avg_attn_109_2.npy",
      "attention_matrices/demo1/avg_attn_109_3.npy",
      "attention_matrices/demo1/avg_attn_109_4.npy",
      "attention_matrices/demo1/avg_attn_109_5.npy",
      "attention_matrices/demo1/avg_attn_109_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 110,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      5,
      5
    ],
    "target": [
      1,
      1,
      4,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nC->B\nD->E\nF->A\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_110.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_110_0.npy",
      "attention_matrices/demo1/avg_attn_110_1.npy",
      "attention_matrices/demo1/avg_attn_110_2.npy",
      "attention_matrices/demo1/avg_attn_110_3.npy",
      "attention_matrices/demo1/avg_attn_110_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 111,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      3,
      4,
      4,
      5
    ],
    "target": [
      1,
      4,
      1,
      0,
      1,
      3,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->B\nA->E\nC->B\nD->A\nE->B\nE->D\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_111.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_111_0.npy",
      "attention_matrices/demo1/avg_attn_111_1.npy",
      "attention_matrices/demo1/avg_attn_111_2.npy",
      "attention_matrices/demo1/avg_attn_111_3.npy",
      "attention_matrices/demo1/avg_attn_111_4.npy",
      "attention_matrices/demo1/avg_attn_111_5.npy",
      "attention_matrices/demo1/avg_attn_111_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 112,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      3,
      3,
      5,
      5
    ],
    "target": [
      3,
      4,
      0,
      2,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->D\nB->E\nD->A\nD->C\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_112.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_112_0.npy",
      "attention_matrices/demo1/avg_attn_112_1.npy",
      "attention_matrices/demo1/avg_attn_112_2.npy",
      "attention_matrices/demo1/avg_attn_112_3.npy",
      "attention_matrices/demo1/avg_attn_112_4.npy",
      "attention_matrices/demo1/avg_attn_112_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 113,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      4,
      5,
      5
    ],
    "target": [
      4,
      1,
      4,
      3,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->E\nC->B\nD->E\nE->D\nF->A\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_113.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_113_0.npy",
      "attention_matrices/demo1/avg_attn_113_1.npy",
      "attention_matrices/demo1/avg_attn_113_2.npy",
      "attention_matrices/demo1/avg_attn_113_3.npy",
      "attention_matrices/demo1/avg_attn_113_4.npy",
      "attention_matrices/demo1/avg_attn_113_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 114,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      4,
      5
    ],
    "target": [
      3,
      3,
      4,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->D\nB->D\nB->E\nE->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_114.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_114_0.npy",
      "attention_matrices/demo1/avg_attn_114_1.npy",
      "attention_matrices/demo1/avg_attn_114_2.npy",
      "attention_matrices/demo1/avg_attn_114_3.npy",
      "attention_matrices/demo1/avg_attn_114_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 115,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      3,
      4,
      4,
      5,
      5
    ],
    "target": [
      2,
      1,
      1,
      2,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->C\nD->B\nE->B\nE->C\nF->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_115.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_115_0.npy",
      "attention_matrices/demo1/avg_attn_115_1.npy",
      "attention_matrices/demo1/avg_attn_115_2.npy",
      "attention_matrices/demo1/avg_attn_115_3.npy",
      "attention_matrices/demo1/avg_attn_115_4.npy",
      "attention_matrices/demo1/avg_attn_115_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 116,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      3,
      3,
      3,
      5,
      5
    ],
    "target": [
      3,
      0,
      2,
      4,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "C->D\nD->A\nD->C\nD->E\nF->A\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_116.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_116_0.npy",
      "attention_matrices/demo1/avg_attn_116_1.npy",
      "attention_matrices/demo1/avg_attn_116_2.npy",
      "attention_matrices/demo1/avg_attn_116_3.npy",
      "attention_matrices/demo1/avg_attn_116_4.npy",
      "attention_matrices/demo1/avg_attn_116_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 117,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      2,
      4,
      5,
      5,
      5
    ],
    "target": [
      3,
      0,
      2,
      0,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->D\nC->A\nE->C\nF->A\nF->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_117.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_117_0.npy",
      "attention_matrices/demo1/avg_attn_117_1.npy",
      "attention_matrices/demo1/avg_attn_117_2.npy",
      "attention_matrices/demo1/avg_attn_117_3.npy",
      "attention_matrices/demo1/avg_attn_117_4.npy",
      "attention_matrices/demo1/avg_attn_117_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 118,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      3,
      3,
      4
    ],
    "target": [
      2,
      3,
      2,
      0,
      0,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nB->C\nC->A\nD->A\nD->C\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_118.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_118_0.npy",
      "attention_matrices/demo1/avg_attn_118_1.npy",
      "attention_matrices/demo1/avg_attn_118_2.npy",
      "attention_matrices/demo1/avg_attn_118_3.npy",
      "attention_matrices/demo1/avg_attn_118_4.npy",
      "attention_matrices/demo1/avg_attn_118_5.npy",
      "attention_matrices/demo1/avg_attn_118_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 119,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      3,
      4,
      5,
      5
    ],
    "target": [
      2,
      4,
      1,
      4,
      0,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nC->B\nD->E\nE->A\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_119.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_119_0.npy",
      "attention_matrices/demo1/avg_attn_119_1.npy",
      "attention_matrices/demo1/avg_attn_119_2.npy",
      "attention_matrices/demo1/avg_attn_119_3.npy",
      "attention_matrices/demo1/avg_attn_119_4.npy",
      "attention_matrices/demo1/avg_attn_119_5.npy",
      "attention_matrices/demo1/avg_attn_119_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 120,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3,
      5
    ],
    "target": [
      2,
      3,
      1,
      2,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->C\nB->D\nC->B\nD->C\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_120.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_120_0.npy",
      "attention_matrices/demo1/avg_attn_120_1.npy",
      "attention_matrices/demo1/avg_attn_120_2.npy",
      "attention_matrices/demo1/avg_attn_120_3.npy",
      "attention_matrices/demo1/avg_attn_120_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 121,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      3,
      3
    ],
    "target": [
      1,
      3,
      2,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nB->C\nD->B\nD->C\n",
    "averaged_attention_matrix_path": "averaged_id_121.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_121_0.npy",
      "attention_matrices/demo1/avg_attn_121_1.npy",
      "attention_matrices/demo1/avg_attn_121_2.npy",
      "attention_matrices/demo1/avg_attn_121_3.npy",
      "attention_matrices/demo1/avg_attn_121_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 122,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3,
      3,
      3,
      5,
      5,
      5
    ],
    "target": [
      4,
      3,
      1,
      1,
      2,
      4,
      0,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->E\nB->D\nC->B\nD->B\nD->C\nD->E\nF->A\nF->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_122.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_122_0.npy",
      "attention_matrices/demo1/avg_attn_122_1.npy",
      "attention_matrices/demo1/avg_attn_122_2.npy",
      "attention_matrices/demo1/avg_attn_122_3.npy",
      "attention_matrices/demo1/avg_attn_122_4.npy",
      "attention_matrices/demo1/avg_attn_122_5.npy",
      "attention_matrices/demo1/avg_attn_122_6.npy",
      "attention_matrices/demo1/avg_attn_122_7.npy",
      "attention_matrices/demo1/avg_attn_122_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 123,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      5,
      5,
      5
    ],
    "target": [
      4,
      4,
      1,
      0,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "B->E\nC->E\nD->B\nF->A\nF->B\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_123.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_123_0.npy",
      "attention_matrices/demo1/avg_attn_123_1.npy",
      "attention_matrices/demo1/avg_attn_123_2.npy",
      "attention_matrices/demo1/avg_attn_123_3.npy",
      "attention_matrices/demo1/avg_attn_123_4.npy",
      "attention_matrices/demo1/avg_attn_123_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 124,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      4,
      4
    ],
    "target": [
      1,
      4,
      0,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->B\nC->E\nD->A\nE->B\nE->C\n",
    "averaged_attention_matrix_path": "averaged_id_124.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_124_0.npy",
      "attention_matrices/demo1/avg_attn_124_1.npy",
      "attention_matrices/demo1/avg_attn_124_2.npy",
      "attention_matrices/demo1/avg_attn_124_3.npy",
      "attention_matrices/demo1/avg_attn_124_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 125,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      3
    ],
    "target": [
      1,
      2,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nB->C\nB->D\nD->B\n",
    "averaged_attention_matrix_path": "averaged_id_125.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_125_0.npy",
      "attention_matrices/demo1/avg_attn_125_1.npy",
      "attention_matrices/demo1/avg_attn_125_2.npy",
      "attention_matrices/demo1/avg_attn_125_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 126,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      3,
      4,
      4,
      5,
      5
    ],
    "target": [
      4,
      2,
      2,
      3,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->E\nD->C\nE->C\nE->D\nF->C\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_126.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_126_0.npy",
      "attention_matrices/demo1/avg_attn_126_1.npy",
      "attention_matrices/demo1/avg_attn_126_2.npy",
      "attention_matrices/demo1/avg_attn_126_3.npy",
      "attention_matrices/demo1/avg_attn_126_4.npy",
      "attention_matrices/demo1/avg_attn_126_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 127,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      4,
      4,
      5,
      5
    ],
    "target": [
      4,
      3,
      1,
      0,
      3,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->E\nC->D\nD->B\nE->A\nE->D\nF->C\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_127.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_127_0.npy",
      "attention_matrices/demo1/avg_attn_127_1.npy",
      "attention_matrices/demo1/avg_attn_127_2.npy",
      "attention_matrices/demo1/avg_attn_127_3.npy",
      "attention_matrices/demo1/avg_attn_127_4.npy",
      "attention_matrices/demo1/avg_attn_127_5.npy",
      "attention_matrices/demo1/avg_attn_127_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 128,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      1,
      3,
      3,
      4,
      5
    ],
    "target": [
      1,
      3,
      4,
      2,
      1,
      2,
      2,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nA->E\nB->C\nD->B\nD->C\nE->C\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_128.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_128_0.npy",
      "attention_matrices/demo1/avg_attn_128_1.npy",
      "attention_matrices/demo1/avg_attn_128_2.npy",
      "attention_matrices/demo1/avg_attn_128_3.npy",
      "attention_matrices/demo1/avg_attn_128_4.npy",
      "attention_matrices/demo1/avg_attn_128_5.npy",
      "attention_matrices/demo1/avg_attn_128_6.npy",
      "attention_matrices/demo1/avg_attn_128_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 129,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      4,
      4,
      5,
      5,
      5
    ],
    "target": [
      2,
      4,
      2,
      4,
      1,
      2,
      1,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->C\nB->E\nE->B\nE->C\nF->B\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_129.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_129_0.npy",
      "attention_matrices/demo1/avg_attn_129_1.npy",
      "attention_matrices/demo1/avg_attn_129_2.npy",
      "attention_matrices/demo1/avg_attn_129_3.npy",
      "attention_matrices/demo1/avg_attn_129_4.npy",
      "attention_matrices/demo1/avg_attn_129_5.npy",
      "attention_matrices/demo1/avg_attn_129_6.npy",
      "attention_matrices/demo1/avg_attn_129_7.npy",
      "attention_matrices/demo1/avg_attn_129_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 130,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      2,
      5
    ],
    "target": [
      2,
      0,
      4,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nC->A\nC->E\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_130.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_130_0.npy",
      "attention_matrices/demo1/avg_attn_130_1.npy",
      "attention_matrices/demo1/avg_attn_130_2.npy",
      "attention_matrices/demo1/avg_attn_130_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 131,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      1,
      3,
      5,
      5
    ],
    "target": [
      0,
      2,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->A\nD->C\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_131.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_131_0.npy",
      "attention_matrices/demo1/avg_attn_131_1.npy",
      "attention_matrices/demo1/avg_attn_131_2.npy",
      "attention_matrices/demo1/avg_attn_131_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 132,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      2,
      2,
      2,
      4,
      5
    ],
    "target": [
      2,
      0,
      1,
      3,
      4,
      1,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nC->A\nC->B\nC->D\nC->E\nE->B\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_132.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_132_0.npy",
      "attention_matrices/demo1/avg_attn_132_1.npy",
      "attention_matrices/demo1/avg_attn_132_2.npy",
      "attention_matrices/demo1/avg_attn_132_3.npy",
      "attention_matrices/demo1/avg_attn_132_4.npy",
      "attention_matrices/demo1/avg_attn_132_5.npy",
      "attention_matrices/demo1/avg_attn_132_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 133,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ]
    ],
    "source": [
      2,
      2,
      4,
      5,
      5
    ],
    "target": [
      0,
      1,
      2,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "C->A\nC->B\nE->C\nF->C\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_133.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_133_0.npy",
      "attention_matrices/demo1/avg_attn_133_1.npy",
      "attention_matrices/demo1/avg_attn_133_2.npy",
      "attention_matrices/demo1/avg_attn_133_3.npy",
      "attention_matrices/demo1/avg_attn_133_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 134,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      2,
      3,
      5
    ],
    "target": [
      3,
      4,
      4,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->D\nB->E\nC->E\nD->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_134.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_134_0.npy",
      "attention_matrices/demo1/avg_attn_134_1.npy",
      "attention_matrices/demo1/avg_attn_134_2.npy",
      "attention_matrices/demo1/avg_attn_134_3.npy",
      "attention_matrices/demo1/avg_attn_134_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 135,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 2,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      4
    ],
    "target": [
      3,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "B->D\nE->A\n",
    "averaged_attention_matrix_path": "averaged_id_135.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_135_0.npy",
      "attention_matrices/demo1/avg_attn_135_1.npy"
    ],
    "num_averaged_samples": 2
  },
  {
    "graph_id": 136,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        1,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      0,
      5,
      5,
      5
    ],
    "target": [
      1,
      2,
      3,
      4,
      0,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nA->C\nA->D\nA->E\nF->A\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_136.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_136_0.npy",
      "attention_matrices/demo1/avg_attn_136_1.npy",
      "attention_matrices/demo1/avg_attn_136_2.npy",
      "attention_matrices/demo1/avg_attn_136_3.npy",
      "attention_matrices/demo1/avg_attn_136_4.npy",
      "attention_matrices/demo1/avg_attn_136_5.npy",
      "attention_matrices/demo1/avg_attn_136_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 137,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      4
    ],
    "target": [
      1,
      4,
      3,
      3,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->B\nA->E\nB->D\nC->D\nE->C\n",
    "averaged_attention_matrix_path": "averaged_id_137.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_137_0.npy",
      "attention_matrices/demo1/avg_attn_137_1.npy",
      "attention_matrices/demo1/avg_attn_137_2.npy",
      "attention_matrices/demo1/avg_attn_137_3.npy",
      "attention_matrices/demo1/avg_attn_137_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 138,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      3,
      4,
      5
    ],
    "target": [
      2,
      0,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->C\nD->A\nE->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_138.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_138_0.npy",
      "attention_matrices/demo1/avg_attn_138_1.npy",
      "attention_matrices/demo1/avg_attn_138_2.npy",
      "attention_matrices/demo1/avg_attn_138_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 139,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      2,
      4,
      4,
      5
    ],
    "target": [
      2,
      4,
      2,
      4,
      3,
      2,
      3,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->C\nB->E\nC->D\nE->C\nE->D\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_139.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_139_0.npy",
      "attention_matrices/demo1/avg_attn_139_1.npy",
      "attention_matrices/demo1/avg_attn_139_2.npy",
      "attention_matrices/demo1/avg_attn_139_3.npy",
      "attention_matrices/demo1/avg_attn_139_4.npy",
      "attention_matrices/demo1/avg_attn_139_5.npy",
      "attention_matrices/demo1/avg_attn_139_6.npy",
      "attention_matrices/demo1/avg_attn_139_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 140,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      4
    ],
    "target": [
      1,
      3,
      2,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nB->C\nE->B\n",
    "averaged_attention_matrix_path": "averaged_id_140.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_140_0.npy",
      "attention_matrices/demo1/avg_attn_140_1.npy",
      "attention_matrices/demo1/avg_attn_140_2.npy",
      "attention_matrices/demo1/avg_attn_140_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 141,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      3,
      4,
      4,
      5,
      5
    ],
    "target": [
      1,
      4,
      2,
      0,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nB->E\nD->C\nE->A\nE->B\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_141.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_141_0.npy",
      "attention_matrices/demo1/avg_attn_141_1.npy",
      "attention_matrices/demo1/avg_attn_141_2.npy",
      "attention_matrices/demo1/avg_attn_141_3.npy",
      "attention_matrices/demo1/avg_attn_141_4.npy",
      "attention_matrices/demo1/avg_attn_141_5.npy",
      "attention_matrices/demo1/avg_attn_141_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 142,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      2,
      3,
      4,
      4,
      5
    ],
    "target": [
      0,
      3,
      1,
      2,
      3,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "C->A\nC->D\nD->B\nE->C\nE->D\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_142.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_142_0.npy",
      "attention_matrices/demo1/avg_attn_142_1.npy",
      "attention_matrices/demo1/avg_attn_142_2.npy",
      "attention_matrices/demo1/avg_attn_142_3.npy",
      "attention_matrices/demo1/avg_attn_142_4.npy",
      "attention_matrices/demo1/avg_attn_142_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 143,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      2,
      3,
      4,
      5
    ],
    "target": [
      2,
      3,
      3,
      4,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nC->D\nC->E\nD->B\nE->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_143.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_143_0.npy",
      "attention_matrices/demo1/avg_attn_143_1.npy",
      "attention_matrices/demo1/avg_attn_143_2.npy",
      "attention_matrices/demo1/avg_attn_143_3.npy",
      "attention_matrices/demo1/avg_attn_143_4.npy",
      "attention_matrices/demo1/avg_attn_143_5.npy",
      "attention_matrices/demo1/avg_attn_143_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 144,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        1,
        0
      ]
    ],
    "source": [
      0,
      0,
      3,
      4,
      4,
      5,
      5,
      5
    ],
    "target": [
      2,
      3,
      0,
      1,
      3,
      0,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nD->A\nE->B\nE->D\nF->A\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_144.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_144_0.npy",
      "attention_matrices/demo1/avg_attn_144_1.npy",
      "attention_matrices/demo1/avg_attn_144_2.npy",
      "attention_matrices/demo1/avg_attn_144_3.npy",
      "attention_matrices/demo1/avg_attn_144_4.npy",
      "attention_matrices/demo1/avg_attn_144_5.npy",
      "attention_matrices/demo1/avg_attn_144_6.npy",
      "attention_matrices/demo1/avg_attn_144_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 145,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      1,
      2,
      2,
      5,
      5,
      5
    ],
    "target": [
      2,
      4,
      1,
      3,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->C\nB->E\nC->B\nC->D\nF->B\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_145.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_145_0.npy",
      "attention_matrices/demo1/avg_attn_145_1.npy",
      "attention_matrices/demo1/avg_attn_145_2.npy",
      "attention_matrices/demo1/avg_attn_145_3.npy",
      "attention_matrices/demo1/avg_attn_145_4.npy",
      "attention_matrices/demo1/avg_attn_145_5.npy",
      "attention_matrices/demo1/avg_attn_145_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 146,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      1,
      3,
      5,
      5,
      5
    ],
    "target": [
      0,
      3,
      4,
      1,
      0,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->A\nB->D\nB->E\nD->B\nF->A\nF->C\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_146.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_146_0.npy",
      "attention_matrices/demo1/avg_attn_146_1.npy",
      "attention_matrices/demo1/avg_attn_146_2.npy",
      "attention_matrices/demo1/avg_attn_146_3.npy",
      "attention_matrices/demo1/avg_attn_146_4.npy",
      "attention_matrices/demo1/avg_attn_146_5.npy",
      "attention_matrices/demo1/avg_attn_146_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 147,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      3,
      4,
      5
    ],
    "target": [
      3,
      4,
      3,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "C->D\nD->E\nE->D\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_147.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_147_0.npy",
      "attention_matrices/demo1/avg_attn_147_1.npy",
      "attention_matrices/demo1/avg_attn_147_2.npy",
      "attention_matrices/demo1/avg_attn_147_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 148,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      4,
      4,
      4,
      5
    ],
    "target": [
      4,
      2,
      0,
      4,
      0,
      1,
      2,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->E\nB->C\nC->A\nC->E\nE->A\nE->B\nE->C\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_148.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_148_0.npy",
      "attention_matrices/demo1/avg_attn_148_1.npy",
      "attention_matrices/demo1/avg_attn_148_2.npy",
      "attention_matrices/demo1/avg_attn_148_3.npy",
      "attention_matrices/demo1/avg_attn_148_4.npy",
      "attention_matrices/demo1/avg_attn_148_5.npy",
      "attention_matrices/demo1/avg_attn_148_6.npy",
      "attention_matrices/demo1/avg_attn_148_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 149,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      2,
      2,
      3,
      4,
      5,
      5
    ],
    "target": [
      1,
      4,
      0,
      4,
      4,
      1,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->B\nA->E\nC->A\nC->E\nD->E\nE->B\nF->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_149.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_149_0.npy",
      "attention_matrices/demo1/avg_attn_149_1.npy",
      "attention_matrices/demo1/avg_attn_149_2.npy",
      "attention_matrices/demo1/avg_attn_149_3.npy",
      "attention_matrices/demo1/avg_attn_149_4.npy",
      "attention_matrices/demo1/avg_attn_149_5.npy",
      "attention_matrices/demo1/avg_attn_149_6.npy",
      "attention_matrices/demo1/avg_attn_149_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 150,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      4,
      4
    ],
    "target": [
      3,
      3,
      4,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->D\nC->D\nD->E\nE->A\nE->B\n",
    "averaged_attention_matrix_path": "averaged_id_150.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_150_0.npy",
      "attention_matrices/demo1/avg_attn_150_1.npy",
      "attention_matrices/demo1/avg_attn_150_2.npy",
      "attention_matrices/demo1/avg_attn_150_3.npy",
      "attention_matrices/demo1/avg_attn_150_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 151,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      3,
      4,
      4,
      5,
      5
    ],
    "target": [
      2,
      2,
      4,
      2,
      0,
      2,
      0,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nB->C\nB->E\nD->C\nE->A\nE->C\nF->A\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_151.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_151_0.npy",
      "attention_matrices/demo1/avg_attn_151_1.npy",
      "attention_matrices/demo1/avg_attn_151_2.npy",
      "attention_matrices/demo1/avg_attn_151_3.npy",
      "attention_matrices/demo1/avg_attn_151_4.npy",
      "attention_matrices/demo1/avg_attn_151_5.npy",
      "attention_matrices/demo1/avg_attn_151_6.npy",
      "attention_matrices/demo1/avg_attn_151_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 152,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      3,
      3,
      4,
      4
    ],
    "target": [
      3,
      2,
      3,
      0,
      2,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->D\nB->C\nB->D\nD->A\nD->C\nE->A\nE->C\n",
    "averaged_attention_matrix_path": "averaged_id_152.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_152_0.npy",
      "attention_matrices/demo1/avg_attn_152_1.npy",
      "attention_matrices/demo1/avg_attn_152_2.npy",
      "attention_matrices/demo1/avg_attn_152_3.npy",
      "attention_matrices/demo1/avg_attn_152_4.npy",
      "attention_matrices/demo1/avg_attn_152_5.npy",
      "attention_matrices/demo1/avg_attn_152_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 153,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 3,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      2,
      4,
      5
    ],
    "target": [
      0,
      0,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "C->A\nE->A\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_153.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_153_0.npy",
      "attention_matrices/demo1/avg_attn_153_1.npy",
      "attention_matrices/demo1/avg_attn_153_2.npy"
    ],
    "num_averaged_samples": 3
  },
  {
    "graph_id": 154,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 11,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      2,
      2,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      2,
      3,
      2,
      1,
      3,
      4,
      2,
      0,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nB->C\nC->B\nC->D\nC->E\nD->C\nE->A\nF->B\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_154.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_154_0.npy",
      "attention_matrices/demo1/avg_attn_154_1.npy",
      "attention_matrices/demo1/avg_attn_154_2.npy",
      "attention_matrices/demo1/avg_attn_154_3.npy",
      "attention_matrices/demo1/avg_attn_154_4.npy",
      "attention_matrices/demo1/avg_attn_154_5.npy",
      "attention_matrices/demo1/avg_attn_154_6.npy",
      "attention_matrices/demo1/avg_attn_154_7.npy",
      "attention_matrices/demo1/avg_attn_154_8.npy",
      "attention_matrices/demo1/avg_attn_154_9.npy",
      "attention_matrices/demo1/avg_attn_154_10.npy"
    ],
    "num_averaged_samples": 11
  },
  {
    "graph_id": 155,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      1,
      2,
      2,
      2,
      4,
      5
    ],
    "target": [
      0,
      2,
      4,
      0,
      1,
      4,
      3,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->A\nB->C\nB->E\nC->A\nC->B\nC->E\nE->D\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_155.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_155_0.npy",
      "attention_matrices/demo1/avg_attn_155_1.npy",
      "attention_matrices/demo1/avg_attn_155_2.npy",
      "attention_matrices/demo1/avg_attn_155_3.npy",
      "attention_matrices/demo1/avg_attn_155_4.npy",
      "attention_matrices/demo1/avg_attn_155_5.npy",
      "attention_matrices/demo1/avg_attn_155_6.npy",
      "attention_matrices/demo1/avg_attn_155_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 156,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      4,
      5
    ],
    "target": [
      2,
      1,
      0,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nC->B\nD->A\nE->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_156.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_156_0.npy",
      "attention_matrices/demo1/avg_attn_156_1.npy",
      "attention_matrices/demo1/avg_attn_156_2.npy",
      "attention_matrices/demo1/avg_attn_156_3.npy",
      "attention_matrices/demo1/avg_attn_156_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 157,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      3,
      3,
      5,
      5
    ],
    "target": [
      2,
      3,
      0,
      4,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->C\nB->D\nD->A\nD->E\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_157.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_157_0.npy",
      "attention_matrices/demo1/avg_attn_157_1.npy",
      "attention_matrices/demo1/avg_attn_157_2.npy",
      "attention_matrices/demo1/avg_attn_157_3.npy",
      "attention_matrices/demo1/avg_attn_157_4.npy",
      "attention_matrices/demo1/avg_attn_157_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 158,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      3,
      3,
      5
    ],
    "target": [
      3,
      4,
      1,
      4,
      0,
      4,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->D\nB->E\nC->B\nC->E\nD->A\nD->E\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_158.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_158_0.npy",
      "attention_matrices/demo1/avg_attn_158_1.npy",
      "attention_matrices/demo1/avg_attn_158_2.npy",
      "attention_matrices/demo1/avg_attn_158_3.npy",
      "attention_matrices/demo1/avg_attn_158_4.npy",
      "attention_matrices/demo1/avg_attn_158_5.npy",
      "attention_matrices/demo1/avg_attn_158_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 159,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 10,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      2,
      3,
      3,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      2,
      0,
      3,
      0,
      1,
      4,
      1,
      0,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->C\nC->A\nC->D\nD->A\nD->B\nD->E\nE->B\nF->A\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_159.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_159_0.npy",
      "attention_matrices/demo1/avg_attn_159_1.npy",
      "attention_matrices/demo1/avg_attn_159_2.npy",
      "attention_matrices/demo1/avg_attn_159_3.npy",
      "attention_matrices/demo1/avg_attn_159_4.npy",
      "attention_matrices/demo1/avg_attn_159_5.npy",
      "attention_matrices/demo1/avg_attn_159_6.npy",
      "attention_matrices/demo1/avg_attn_159_7.npy",
      "attention_matrices/demo1/avg_attn_159_8.npy",
      "attention_matrices/demo1/avg_attn_159_9.npy"
    ],
    "num_averaged_samples": 10
  },
  {
    "graph_id": 160,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      2,
      5,
      5
    ],
    "target": [
      2,
      1,
      3,
      2,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nC->B\nC->D\nF->C\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_160.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_160_0.npy",
      "attention_matrices/demo1/avg_attn_160_1.npy",
      "attention_matrices/demo1/avg_attn_160_2.npy",
      "attention_matrices/demo1/avg_attn_160_3.npy",
      "attention_matrices/demo1/avg_attn_160_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 161,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      3,
      3,
      4,
      5
    ],
    "target": [
      1,
      4,
      2,
      4,
      0,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nB->E\nD->C\nD->E\nE->A\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_161.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_161_0.npy",
      "attention_matrices/demo1/avg_attn_161_1.npy",
      "attention_matrices/demo1/avg_attn_161_2.npy",
      "attention_matrices/demo1/avg_attn_161_3.npy",
      "attention_matrices/demo1/avg_attn_161_4.npy",
      "attention_matrices/demo1/avg_attn_161_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 162,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 3,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      2,
      3,
      3
    ],
    "target": [
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "C->B\nD->C\nD->E\n",
    "averaged_attention_matrix_path": "averaged_id_162.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_162_0.npy",
      "attention_matrices/demo1/avg_attn_162_1.npy",
      "attention_matrices/demo1/avg_attn_162_2.npy"
    ],
    "num_averaged_samples": 3
  },
  {
    "graph_id": 163,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3,
      3,
      4,
      4,
      5
    ],
    "target": [
      2,
      4,
      3,
      1,
      2,
      1,
      2,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nB->E\nC->D\nD->B\nD->C\nE->B\nE->C\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_163.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_163_0.npy",
      "attention_matrices/demo1/avg_attn_163_1.npy",
      "attention_matrices/demo1/avg_attn_163_2.npy",
      "attention_matrices/demo1/avg_attn_163_3.npy",
      "attention_matrices/demo1/avg_attn_163_4.npy",
      "attention_matrices/demo1/avg_attn_163_5.npy",
      "attention_matrices/demo1/avg_attn_163_6.npy",
      "attention_matrices/demo1/avg_attn_163_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 164,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 10,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        1,
        1,
        1,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      3,
      4,
      4,
      5,
      5,
      5,
      5
    ],
    "target": [
      4,
      4,
      1,
      4,
      0,
      3,
      0,
      2,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->E\nC->E\nD->B\nD->E\nE->A\nE->D\nF->A\nF->C\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_164.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_164_0.npy",
      "attention_matrices/demo1/avg_attn_164_1.npy",
      "attention_matrices/demo1/avg_attn_164_2.npy",
      "attention_matrices/demo1/avg_attn_164_3.npy",
      "attention_matrices/demo1/avg_attn_164_4.npy",
      "attention_matrices/demo1/avg_attn_164_5.npy",
      "attention_matrices/demo1/avg_attn_164_6.npy",
      "attention_matrices/demo1/avg_attn_164_7.npy",
      "attention_matrices/demo1/avg_attn_164_8.npy",
      "attention_matrices/demo1/avg_attn_164_9.npy"
    ],
    "num_averaged_samples": 10
  },
  {
    "graph_id": 165,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      3,
      3,
      4,
      5
    ],
    "target": [
      1,
      2,
      0,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "D->B\nD->C\nE->A\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_165.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_165_0.npy",
      "attention_matrices/demo1/avg_attn_165_1.npy",
      "attention_matrices/demo1/avg_attn_165_2.npy",
      "attention_matrices/demo1/avg_attn_165_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 166,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      3,
      3,
      4,
      4,
      5
    ],
    "target": [
      2,
      0,
      1,
      0,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->C\nD->A\nD->B\nE->A\nE->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_166.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_166_0.npy",
      "attention_matrices/demo1/avg_attn_166_1.npy",
      "attention_matrices/demo1/avg_attn_166_2.npy",
      "attention_matrices/demo1/avg_attn_166_3.npy",
      "attention_matrices/demo1/avg_attn_166_4.npy",
      "attention_matrices/demo1/avg_attn_166_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 167,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      3,
      3,
      4,
      5
    ],
    "target": [
      4,
      2,
      1,
      4,
      0,
      4,
      0,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->E\nB->C\nC->B\nC->E\nD->A\nD->E\nE->A\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_167.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_167_0.npy",
      "attention_matrices/demo1/avg_attn_167_1.npy",
      "attention_matrices/demo1/avg_attn_167_2.npy",
      "attention_matrices/demo1/avg_attn_167_3.npy",
      "attention_matrices/demo1/avg_attn_167_4.npy",
      "attention_matrices/demo1/avg_attn_167_5.npy",
      "attention_matrices/demo1/avg_attn_167_6.npy",
      "attention_matrices/demo1/avg_attn_167_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 168,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      0,
      0,
      4,
      2,
      0,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->A\nC->A\nD->E\nE->C\nF->A\nF->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_168.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_168_0.npy",
      "attention_matrices/demo1/avg_attn_168_1.npy",
      "attention_matrices/demo1/avg_attn_168_2.npy",
      "attention_matrices/demo1/avg_attn_168_3.npy",
      "attention_matrices/demo1/avg_attn_168_4.npy",
      "attention_matrices/demo1/avg_attn_168_5.npy",
      "attention_matrices/demo1/avg_attn_168_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 169,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      3,
      3,
      4,
      5
    ],
    "target": [
      0,
      4,
      2,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "D->A\nD->E\nE->C\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_169.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_169_0.npy",
      "attention_matrices/demo1/avg_attn_169_1.npy",
      "attention_matrices/demo1/avg_attn_169_2.npy",
      "attention_matrices/demo1/avg_attn_169_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 170,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      2,
      4,
      5,
      5
    ],
    "target": [
      3,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "C",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "C->D\nE->B\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_170.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_170_0.npy",
      "attention_matrices/demo1/avg_attn_170_1.npy",
      "attention_matrices/demo1/avg_attn_170_2.npy",
      "attention_matrices/demo1/avg_attn_170_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 171,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      3,
      4,
      4,
      5
    ],
    "target": [
      4,
      0,
      1,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->E\nD->A\nE->B\nE->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_171.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_171_0.npy",
      "attention_matrices/demo1/avg_attn_171_1.npy",
      "attention_matrices/demo1/avg_attn_171_2.npy",
      "attention_matrices/demo1/avg_attn_171_3.npy",
      "attention_matrices/demo1/avg_attn_171_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 172,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      2,
      4,
      4,
      5,
      5,
      5
    ],
    "target": [
      3,
      0,
      1,
      2,
      0,
      1,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->D\nC->A\nE->B\nE->C\nF->A\nF->B\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_172.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_172_0.npy",
      "attention_matrices/demo1/avg_attn_172_1.npy",
      "attention_matrices/demo1/avg_attn_172_2.npy",
      "attention_matrices/demo1/avg_attn_172_3.npy",
      "attention_matrices/demo1/avg_attn_172_4.npy",
      "attention_matrices/demo1/avg_attn_172_5.npy",
      "attention_matrices/demo1/avg_attn_172_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 173,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 10,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      1,
      1,
      2,
      3,
      3,
      3,
      5
    ],
    "target": [
      1,
      3,
      4,
      2,
      3,
      0,
      0,
      1,
      4,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nA->E\nB->C\nB->D\nC->A\nD->A\nD->B\nD->E\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_173.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_173_0.npy",
      "attention_matrices/demo1/avg_attn_173_1.npy",
      "attention_matrices/demo1/avg_attn_173_2.npy",
      "attention_matrices/demo1/avg_attn_173_3.npy",
      "attention_matrices/demo1/avg_attn_173_4.npy",
      "attention_matrices/demo1/avg_attn_173_5.npy",
      "attention_matrices/demo1/avg_attn_173_6.npy",
      "attention_matrices/demo1/avg_attn_173_7.npy",
      "attention_matrices/demo1/avg_attn_173_8.npy",
      "attention_matrices/demo1/avg_attn_173_9.npy"
    ],
    "num_averaged_samples": 10
  },
  {
    "graph_id": 174,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      3,
      4,
      5
    ],
    "target": [
      2,
      4,
      4,
      4,
      2,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->E\nD->E\nE->C\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_174.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_174_0.npy",
      "attention_matrices/demo1/avg_attn_174_1.npy",
      "attention_matrices/demo1/avg_attn_174_2.npy",
      "attention_matrices/demo1/avg_attn_174_3.npy",
      "attention_matrices/demo1/avg_attn_174_4.npy",
      "attention_matrices/demo1/avg_attn_174_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 175,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      2,
      3,
      4
    ],
    "target": [
      1,
      3,
      2,
      3,
      3,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->B\nA->D\nB->C\nB->D\nC->D\nD->A\nE->C\n",
    "averaged_attention_matrix_path": "averaged_id_175.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_175_0.npy",
      "attention_matrices/demo1/avg_attn_175_1.npy",
      "attention_matrices/demo1/avg_attn_175_2.npy",
      "attention_matrices/demo1/avg_attn_175_3.npy",
      "attention_matrices/demo1/avg_attn_175_4.npy",
      "attention_matrices/demo1/avg_attn_175_5.npy",
      "attention_matrices/demo1/avg_attn_175_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 176,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        1,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      2,
      3,
      2,
      1,
      2,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->C\nC->D\nD->C\nE->B\nF->C\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_176.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_176_0.npy",
      "attention_matrices/demo1/avg_attn_176_1.npy",
      "attention_matrices/demo1/avg_attn_176_2.npy",
      "attention_matrices/demo1/avg_attn_176_3.npy",
      "attention_matrices/demo1/avg_attn_176_4.npy",
      "attention_matrices/demo1/avg_attn_176_5.npy",
      "attention_matrices/demo1/avg_attn_176_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 177,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      2,
      3,
      3,
      4,
      5,
      5
    ],
    "target": [
      2,
      0,
      1,
      4,
      2,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nC->A\nD->B\nD->E\nE->C\nF->B\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_177.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_177_0.npy",
      "attention_matrices/demo1/avg_attn_177_1.npy",
      "attention_matrices/demo1/avg_attn_177_2.npy",
      "attention_matrices/demo1/avg_attn_177_3.npy",
      "attention_matrices/demo1/avg_attn_177_4.npy",
      "attention_matrices/demo1/avg_attn_177_5.npy",
      "attention_matrices/demo1/avg_attn_177_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 178,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      3,
      3,
      4,
      4,
      5
    ],
    "target": [
      2,
      4,
      4,
      1,
      2,
      0,
      3,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->E\nD->B\nD->C\nE->A\nE->D\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_178.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_178_0.npy",
      "attention_matrices/demo1/avg_attn_178_1.npy",
      "attention_matrices/demo1/avg_attn_178_2.npy",
      "attention_matrices/demo1/avg_attn_178_3.npy",
      "attention_matrices/demo1/avg_attn_178_4.npy",
      "attention_matrices/demo1/avg_attn_178_5.npy",
      "attention_matrices/demo1/avg_attn_178_6.npy",
      "attention_matrices/demo1/avg_attn_178_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 179,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      3
    ],
    "target": [
      2,
      3,
      4,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->C\nA->D\nA->E\nD->C\n",
    "averaged_attention_matrix_path": "averaged_id_179.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_179_0.npy",
      "attention_matrices/demo1/avg_attn_179_1.npy",
      "attention_matrices/demo1/avg_attn_179_2.npy",
      "attention_matrices/demo1/avg_attn_179_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 180,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        1,
        1,
        0
      ]
    ],
    "source": [
      0,
      1,
      4,
      5,
      5,
      5
    ],
    "target": [
      3,
      3,
      1,
      2,
      3,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->D\nB->D\nE->B\nF->C\nF->D\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_180.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_180_0.npy",
      "attention_matrices/demo1/avg_attn_180_1.npy",
      "attention_matrices/demo1/avg_attn_180_2.npy",
      "attention_matrices/demo1/avg_attn_180_3.npy",
      "attention_matrices/demo1/avg_attn_180_4.npy",
      "attention_matrices/demo1/avg_attn_180_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 181,
    "max_nodes": 6,
    "num_nodes": 2,
    "num_edges": 1,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ]
    ],
    "source": [
      5
    ],
    "target": [
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "F->E\n",
    "averaged_attention_matrix_path": "averaged_id_181.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_181_0.npy"
    ],
    "num_averaged_samples": 1
  },
  {
    "graph_id": 182,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 9,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      1,
      2,
      3,
      4,
      2,
      0,
      1,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nA->C\nB->D\nC->E\nD->C\nE->A\nF->B\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_182.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_182_0.npy",
      "attention_matrices/demo1/avg_attn_182_1.npy",
      "attention_matrices/demo1/avg_attn_182_2.npy",
      "attention_matrices/demo1/avg_attn_182_3.npy",
      "attention_matrices/demo1/avg_attn_182_4.npy",
      "attention_matrices/demo1/avg_attn_182_5.npy",
      "attention_matrices/demo1/avg_attn_182_6.npy",
      "attention_matrices/demo1/avg_attn_182_7.npy",
      "attention_matrices/demo1/avg_attn_182_8.npy"
    ],
    "num_averaged_samples": 9
  },
  {
    "graph_id": 183,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 11,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        1,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      0,
      1,
      2,
      2,
      2,
      4,
      5,
      5
    ],
    "target": [
      1,
      2,
      3,
      4,
      3,
      0,
      1,
      3,
      1,
      0,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->B\nA->C\nA->D\nA->E\nB->D\nC->A\nC->B\nC->D\nE->B\nF->A\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_183.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_183_0.npy",
      "attention_matrices/demo1/avg_attn_183_1.npy",
      "attention_matrices/demo1/avg_attn_183_2.npy",
      "attention_matrices/demo1/avg_attn_183_3.npy",
      "attention_matrices/demo1/avg_attn_183_4.npy",
      "attention_matrices/demo1/avg_attn_183_5.npy",
      "attention_matrices/demo1/avg_attn_183_6.npy",
      "attention_matrices/demo1/avg_attn_183_7.npy",
      "attention_matrices/demo1/avg_attn_183_8.npy",
      "attention_matrices/demo1/avg_attn_183_9.npy",
      "attention_matrices/demo1/avg_attn_183_10.npy"
    ],
    "num_averaged_samples": 11
  },
  {
    "graph_id": 184,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      2,
      3,
      4,
      5
    ],
    "target": [
      0,
      4,
      1,
      0,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "B->A\nB->E\nC->B\nD->A\nE->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_184.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_184_0.npy",
      "attention_matrices/demo1/avg_attn_184_1.npy",
      "attention_matrices/demo1/avg_attn_184_2.npy",
      "attention_matrices/demo1/avg_attn_184_3.npy",
      "attention_matrices/demo1/avg_attn_184_4.npy",
      "attention_matrices/demo1/avg_attn_184_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 185,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      3,
      4
    ],
    "target": [
      3,
      3,
      4,
      3,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->D\nB->D\nB->E\nC->D\nD->B\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_185.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_185_0.npy",
      "attention_matrices/demo1/avg_attn_185_1.npy",
      "attention_matrices/demo1/avg_attn_185_2.npy",
      "attention_matrices/demo1/avg_attn_185_3.npy",
      "attention_matrices/demo1/avg_attn_185_4.npy",
      "attention_matrices/demo1/avg_attn_185_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 186,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 11,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        1,
        1,
        1,
        1,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      0,
      0,
      1,
      1,
      2,
      2,
      3,
      3,
      3
    ],
    "target": [
      1,
      2,
      3,
      4,
      2,
      4,
      1,
      3,
      0,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "B",
      "\u010a",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "A->B\nA->C\nA->D\nA->E\nB->C\nB->E\nC->B\nC->D\nD->A\nD->C\nD->E\n",
    "averaged_attention_matrix_path": "averaged_id_186.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_186_0.npy",
      "attention_matrices/demo1/avg_attn_186_1.npy",
      "attention_matrices/demo1/avg_attn_186_2.npy",
      "attention_matrices/demo1/avg_attn_186_3.npy",
      "attention_matrices/demo1/avg_attn_186_4.npy",
      "attention_matrices/demo1/avg_attn_186_5.npy",
      "attention_matrices/demo1/avg_attn_186_6.npy",
      "attention_matrices/demo1/avg_attn_186_7.npy",
      "attention_matrices/demo1/avg_attn_186_8.npy",
      "attention_matrices/demo1/avg_attn_186_9.npy",
      "attention_matrices/demo1/avg_attn_186_10.npy"
    ],
    "num_averaged_samples": 11
  },
  {
    "graph_id": 187,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 4,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      1,
      2,
      3
    ],
    "target": [
      3,
      4,
      3,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "B->D\nB->E\nC->D\nD->B\n",
    "averaged_attention_matrix_path": "averaged_id_187.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_187_0.npy",
      "attention_matrices/demo1/avg_attn_187_1.npy",
      "attention_matrices/demo1/avg_attn_187_2.npy",
      "attention_matrices/demo1/avg_attn_187_3.npy"
    ],
    "num_averaged_samples": 4
  },
  {
    "graph_id": 188,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      4,
      4,
      5,
      5
    ],
    "target": [
      3,
      1,
      3,
      0,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->D\nE->B\nE->D\nF->A\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_188.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_188_0.npy",
      "attention_matrices/demo1/avg_attn_188_1.npy",
      "attention_matrices/demo1/avg_attn_188_2.npy",
      "attention_matrices/demo1/avg_attn_188_3.npy",
      "attention_matrices/demo1/avg_attn_188_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 189,
    "max_nodes": 6,
    "num_nodes": 4,
    "num_edges": 3,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      4,
      5,
      5
    ],
    "target": [
      0,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "E->A\nF->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_189.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_189_0.npy",
      "attention_matrices/demo1/avg_attn_189_1.npy",
      "attention_matrices/demo1/avg_attn_189_2.npy"
    ],
    "num_averaged_samples": 3
  },
  {
    "graph_id": 190,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      3
    ],
    "target": [
      3,
      4,
      0,
      1,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->D\nA->E\nB->A\nC->B\nD->C\n",
    "averaged_attention_matrix_path": "averaged_id_190.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_190_0.npy",
      "attention_matrices/demo1/avg_attn_190_1.npy",
      "attention_matrices/demo1/avg_attn_190_2.npy",
      "attention_matrices/demo1/avg_attn_190_3.npy",
      "attention_matrices/demo1/avg_attn_190_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 191,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      2,
      4,
      4,
      4,
      5
    ],
    "target": [
      2,
      4,
      0,
      3,
      0,
      2,
      3,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->A\nC->D\nE->A\nE->C\nE->D\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_191.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_191_0.npy",
      "attention_matrices/demo1/avg_attn_191_1.npy",
      "attention_matrices/demo1/avg_attn_191_2.npy",
      "attention_matrices/demo1/avg_attn_191_3.npy",
      "attention_matrices/demo1/avg_attn_191_4.npy",
      "attention_matrices/demo1/avg_attn_191_5.npy",
      "attention_matrices/demo1/avg_attn_191_6.npy",
      "attention_matrices/demo1/avg_attn_191_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 192,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      3,
      4,
      5
    ],
    "target": [
      4,
      4,
      1,
      3,
      0,
      2,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->E\nB->E\nC->B\nC->D\nD->A\nE->C\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_192.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_192_0.npy",
      "attention_matrices/demo1/avg_attn_192_1.npy",
      "attention_matrices/demo1/avg_attn_192_2.npy",
      "attention_matrices/demo1/avg_attn_192_3.npy",
      "attention_matrices/demo1/avg_attn_192_4.npy",
      "attention_matrices/demo1/avg_attn_192_5.npy",
      "attention_matrices/demo1/avg_attn_192_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 193,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ]
    ],
    "source": [
      1,
      2,
      3,
      3,
      4,
      5,
      5,
      5
    ],
    "target": [
      3,
      3,
      0,
      2,
      0,
      0,
      2,
      4
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "D",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a",
      "F",
      "->",
      "E",
      "\u010a"
    ],
    "prompt": "B->D\nC->D\nD->A\nD->C\nE->A\nF->A\nF->C\nF->E\n",
    "averaged_attention_matrix_path": "averaged_id_193.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_193_0.npy",
      "attention_matrices/demo1/avg_attn_193_1.npy",
      "attention_matrices/demo1/avg_attn_193_2.npy",
      "attention_matrices/demo1/avg_attn_193_3.npy",
      "attention_matrices/demo1/avg_attn_193_4.npy",
      "attention_matrices/demo1/avg_attn_193_5.npy",
      "attention_matrices/demo1/avg_attn_193_6.npy",
      "attention_matrices/demo1/avg_attn_193_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 194,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        1,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      1,
      2,
      2,
      3,
      4,
      4,
      4
    ],
    "target": [
      4,
      0,
      3,
      2,
      0,
      1,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "D",
      "->",
      "C",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "E",
      "->",
      "B",
      "\u010a",
      "E",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "B->E\nC->A\nC->D\nD->C\nE->A\nE->B\nE->D\n",
    "averaged_attention_matrix_path": "averaged_id_194.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_194_0.npy",
      "attention_matrices/demo1/avg_attn_194_1.npy",
      "attention_matrices/demo1/avg_attn_194_2.npy",
      "attention_matrices/demo1/avg_attn_194_3.npy",
      "attention_matrices/demo1/avg_attn_194_4.npy",
      "attention_matrices/demo1/avg_attn_194_5.npy",
      "attention_matrices/demo1/avg_attn_194_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 195,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 6,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      3,
      5
    ],
    "target": [
      3,
      3,
      4,
      0,
      1,
      1
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "D",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "B",
      "\u010a",
      "F",
      "->",
      "B",
      "\u010a"
    ],
    "prompt": "A->D\nB->D\nB->E\nC->A\nD->B\nF->B\n",
    "averaged_attention_matrix_path": "averaged_id_195.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_195_0.npy",
      "attention_matrices/demo1/avg_attn_195_1.npy",
      "attention_matrices/demo1/avg_attn_195_2.npy",
      "attention_matrices/demo1/avg_attn_195_3.npy",
      "attention_matrices/demo1/avg_attn_195_4.npy",
      "attention_matrices/demo1/avg_attn_195_5.npy"
    ],
    "num_averaged_samples": 6
  },
  {
    "graph_id": 196,
    "max_nodes": 6,
    "num_nodes": 5,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      1,
      2,
      3
    ],
    "target": [
      2,
      2,
      4,
      0,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nB->C\nB->E\nC->A\nD->A\n",
    "averaged_attention_matrix_path": "averaged_id_196.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_196_0.npy",
      "attention_matrices/demo1/avg_attn_196_1.npy",
      "attention_matrices/demo1/avg_attn_196_2.npy",
      "attention_matrices/demo1/avg_attn_196_3.npy",
      "attention_matrices/demo1/avg_attn_196_4.npy"
    ],
    "num_averaged_samples": 5
  },
  {
    "graph_id": 197,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 7,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        1,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      3,
      3,
      4,
      5
    ],
    "target": [
      4,
      4,
      0,
      0,
      4,
      0,
      3
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "D",
      "->",
      "E",
      "\u010a",
      "E",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "D",
      "\u010a"
    ],
    "prompt": "A->E\nB->E\nC->A\nD->A\nD->E\nE->A\nF->D\n",
    "averaged_attention_matrix_path": "averaged_id_197.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_197_0.npy",
      "attention_matrices/demo1/avg_attn_197_1.npy",
      "attention_matrices/demo1/avg_attn_197_2.npy",
      "attention_matrices/demo1/avg_attn_197_3.npy",
      "attention_matrices/demo1/avg_attn_197_4.npy",
      "attention_matrices/demo1/avg_attn_197_5.npy",
      "attention_matrices/demo1/avg_attn_197_6.npy"
    ],
    "num_averaged_samples": 7
  },
  {
    "graph_id": 198,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 8,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        1,
        0,
        1,
        0
      ],
      [
        1,
        0,
        1,
        0,
        1,
        0
      ],
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      0,
      1,
      1,
      1,
      2,
      3,
      5
    ],
    "target": [
      2,
      4,
      0,
      2,
      4,
      4,
      0,
      0
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "C",
      "\u010a",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "B",
      "->",
      "C",
      "\u010a",
      "B",
      "->",
      "E",
      "\u010a",
      "C",
      "->",
      "E",
      "\u010a",
      "D",
      "->",
      "A",
      "\u010a",
      "F",
      "->",
      "A",
      "\u010a"
    ],
    "prompt": "A->C\nA->E\nB->A\nB->C\nB->E\nC->E\nD->A\nF->A\n",
    "averaged_attention_matrix_path": "averaged_id_198.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_198_0.npy",
      "attention_matrices/demo1/avg_attn_198_1.npy",
      "attention_matrices/demo1/avg_attn_198_2.npy",
      "attention_matrices/demo1/avg_attn_198_3.npy",
      "attention_matrices/demo1/avg_attn_198_4.npy",
      "attention_matrices/demo1/avg_attn_198_5.npy",
      "attention_matrices/demo1/avg_attn_198_6.npy",
      "attention_matrices/demo1/avg_attn_198_7.npy"
    ],
    "num_averaged_samples": 8
  },
  {
    "graph_id": 199,
    "max_nodes": 6,
    "num_nodes": 6,
    "num_edges": 5,
    "connection_probability": 0.25,
    "layer": "0",
    "head": "all",
    "gt_adjacency": [
      [
        0,
        0,
        0,
        0,
        1,
        0
      ],
      [
        1,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        1,
        0,
        1,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        0,
        0,
        0,
        0
      ],
      [
        0,
        0,
        1,
        0,
        0,
        0
      ]
    ],
    "source": [
      0,
      1,
      2,
      2,
      5
    ],
    "target": [
      4,
      0,
      1,
      3,
      2
    ],
    "tokens": [
      "<|begin_of_text|>",
      "A",
      "->",
      "E",
      "\u010a",
      "B",
      "->",
      "A",
      "\u010a",
      "C",
      "->",
      "B",
      "\u010a",
      "C",
      "->",
      "D",
      "\u010a",
      "F",
      "->",
      "C",
      "\u010a"
    ],
    "prompt": "A->E\nB->A\nC->B\nC->D\nF->C\n",
    "averaged_attention_matrix_path": "averaged_id_199.pt",
    "original_attention_matrix_paths": [
      "attention_matrices/demo1/avg_attn_199_0.npy",
      "attention_matrices/demo1/avg_attn_199_1.npy",
      "attention_matrices/demo1/avg_attn_199_2.npy",
      "attention_matrices/demo1/avg_attn_199_3.npy",
      "attention_matrices/demo1/avg_attn_199_4.npy"
    ],
    "num_averaged_samples": 5
  }
]